{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Necessary Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preview:\n",
      "   UserID  MovieID  Rating                 Title\n",
      "0       1      122     5.0      Boomerang (1992)\n",
      "1       1      185     5.0       Net, The (1995)\n",
      "2       1      231     5.0  Dumb & Dumber (1994)\n",
      "3       1      292     5.0       Outbreak (1995)\n",
      "4       1      316     5.0       Stargate (1994)\n",
      "\n",
      "Dataset shape: (10000054, 4)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and Preview the Dataset\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:/Users/anujp/OneDrive/Desktop/MovieRecommendations/data/Final_data/Final_data.csv\")\n",
    "\n",
    "# Preview the first few rows\n",
    "print(\"Data preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check the shape of the dataset\n",
    "print(\"\\nDataset shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 69878\n",
      "Number of items: 10677\n",
      "\n",
      "Prepared data for modeling:\n",
      "   user  item  Rating_normalized\n",
      "0     0   120                1.0\n",
      "1     0   183                1.0\n",
      "2     0   228                1.0\n",
      "3     0   289                1.0\n",
      "4     0   313                1.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Preprocessing\n",
    "\n",
    "# Normalize the ratings to [0, 1]\n",
    "max_rating = df['Rating'].max()\n",
    "min_rating = df['Rating'].min()\n",
    "df['Rating_normalized'] = (df['Rating'] - min_rating) / (max_rating - min_rating)\n",
    "\n",
    "# Encode UserID and MovieID to indices starting from 0\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "df['user'] = user_encoder.fit_transform(df['UserID'])\n",
    "\n",
    "item_encoder = LabelEncoder()\n",
    "df['item'] = item_encoder.fit_transform(df['MovieID'])\n",
    "\n",
    "num_users = df['user'].nunique()\n",
    "num_items = df['item'].nunique()\n",
    "\n",
    "print(f\"Number of users: {num_users}\")\n",
    "print(f\"Number of items: {num_items}\")\n",
    "\n",
    "# Create a new DataFrame with the necessary columns\n",
    "df_model = df[['user', 'item', 'Rating_normalized']]\n",
    "\n",
    "# Check the head of the new DataFrame\n",
    "print(\"\\nPrepared data for modeling:\")\n",
    "print(df_model.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (8000043, 3)\n",
      "Validation data shape: (1000005, 3)\n",
      "Testing data shape: (1000006, 3)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Split the Data into Training, Validation, and Testing Sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, split the data into training and temp (validation + testing)\n",
    "train_data, temp_data = train_test_split(df_model, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then split the temp data equally into validation and testing\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create PyTorch Datasets and DataLoaders\n",
    "\n",
    "class NCFDataset(Dataset):\n",
    "    def __init__(self, user_tensor, item_tensor, target_tensor):\n",
    "        self.user_tensor = user_tensor\n",
    "        self.item_tensor = item_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.user_tensor.size(0)\n",
    "\n",
    "# Convert data to tensors\n",
    "train_users = torch.tensor(train_data['user'].values, dtype=torch.long)\n",
    "train_items = torch.tensor(train_data['item'].values, dtype=torch.long)\n",
    "train_ratings = torch.tensor(train_data['Rating_normalized'].values, dtype=torch.float32)\n",
    "\n",
    "val_users = torch.tensor(val_data['user'].values, dtype=torch.long)\n",
    "val_items = torch.tensor(val_data['item'].values, dtype=torch.long)\n",
    "val_ratings = torch.tensor(val_data['Rating_normalized'].values, dtype=torch.float32)\n",
    "\n",
    "test_users = torch.tensor(test_data['user'].values, dtype=torch.long)\n",
    "test_items = torch.tensor(test_data['item'].values, dtype=torch.long)\n",
    "test_ratings = torch.tensor(test_data['Rating_normalized'].values, dtype=torch.float32)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NCFDataset(train_users, train_items, train_ratings)\n",
    "val_dataset = NCFDataset(val_users, val_items, val_ratings)\n",
    "test_dataset = NCFDataset(test_users, test_items, test_ratings)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 2048  # Adjust based on memory constraints\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Define the NCF Model with Improvements\n",
    "\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=128, hidden_layers=[256, 128, 64], dropout_rate=0.2):\n",
    "        super(NCF, self).__init__()\n",
    "        # User and Item embeddings\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Define MLP layers\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        input_size = embedding_dim * 2  # Since we concatenate user and item embeddings\n",
    "        for hidden_size in hidden_layers:\n",
    "            self.fc_layers.append(nn.Linear(input_size, hidden_size))\n",
    "            self.fc_layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            self.fc_layers.append(nn.LeakyReLU())\n",
    "            self.fc_layers.append(self.dropout)\n",
    "            input_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(input_size, 1)\n",
    "        \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        # Get embeddings\n",
    "        user_embedding = self.user_embedding(user_indices)\n",
    "        item_embedding = self.item_embedding(item_indices)\n",
    "        \n",
    "        # Concatenate user and item embeddings\n",
    "        vector = torch.cat([user_embedding, item_embedding], dim=-1)\n",
    "        \n",
    "        # Pass through MLP layers\n",
    "        for layer in self.fc_layers:\n",
    "            vector = layer(vector)\n",
    "            \n",
    "        # Output layer\n",
    "        rating = self.output_layer(vector)\n",
    "        rating = torch.sigmoid(rating)  # Ensure output is between 0 and 1\n",
    "        return rating.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Initialize the Model, Define Loss Function and Optimizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = NCF(num_users, num_items).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.BCELoss()  # Binary Cross Entropy due to sigmoid output\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|██████████| 3907/3907 [04:33<00:00, 14.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Train the Model with Early Stopping\n",
    "\n",
    "epochs = 25  # Adjust based on your computational resources\n",
    "model_save_path = 'ncf_model.pth'\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 3  # Number of epochs to wait before stopping\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_users, batch_items, batch_ratings in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        batch_users = batch_users.to(device)\n",
    "        batch_items = batch_items.to(device)\n",
    "        batch_ratings = batch_ratings.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_users, batch_items)\n",
    "        loss = loss_function(predictions, batch_ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * batch_users.size(0)\n",
    "        \n",
    "    avg_loss = total_loss / len(train_dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for val_users_batch, val_items_batch, val_ratings_batch in val_loader:\n",
    "            val_users_batch = val_users_batch.to(device)\n",
    "            val_items_batch = val_items_batch.to(device)\n",
    "            val_ratings_batch = val_ratings_batch.to(device)\n",
    "            val_predictions = model(val_users_batch, val_items_batch)\n",
    "            batch_loss = loss_function(val_predictions, val_ratings_batch)\n",
    "            val_loss += batch_loss.item() * val_users_batch.size(0)\n",
    "    avg_val_loss = val_loss / len(val_dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Early Stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Load the Best Model for Inference\n",
    "\n",
    "# Initialize the model architecture\n",
    "loaded_model = NCF(num_users, num_items).to(device)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "loaded_model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "loaded_model.eval()\n",
    "print(\"Best model loaded and ready for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Evaluate the Loaded Model on the Test Set\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_users = test_users.to(device)\n",
    "    test_items = test_items.to(device)\n",
    "    test_ratings = test_ratings.to(device)\n",
    "    \n",
    "    predictions = loaded_model(test_users, test_items)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    mse = mse_loss(predictions, test_ratings)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    mae = torch.mean(torch.abs(predictions - test_ratings))\n",
    "    \n",
    "    print(f\"Test MSE: {mse.item():.4f}\")\n",
    "    print(f\"Test RMSE: {rmse.item():.4f}\")\n",
    "    print(f\"Test MAE: {mae.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Prepare Data for Ranking Metrics\n",
    "\n",
    "# Get unique users and items\n",
    "unique_users = df_model['user'].unique()\n",
    "unique_items = df_model['item'].unique()\n",
    "\n",
    "# Create a dictionary of actual items rated by each user in the test set\n",
    "test_user_item_dict = test_data.groupby('user')['item'].apply(set).to_dict()\n",
    "\n",
    "# Create a dictionary of items rated by each user in the training and validation sets\n",
    "train_val_data = pd.concat([train_data, val_data])\n",
    "train_user_item_dict = train_val_data.groupby('user')['item'].apply(set).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save the Trained Model\n",
    "\n",
    "model_save_path = 'C:/Users/anujp/OneDrive/Desktop/MovieRecommendations/models/neural_collaborative_filtering/ncf_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
