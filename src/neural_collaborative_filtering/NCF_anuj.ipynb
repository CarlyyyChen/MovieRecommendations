{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries and Read Data"
      ],
      "metadata": {
        "id": "Ux3QeCzQ2QCX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fsp31lsDovHR",
        "outputId": "2bf470ef-1746-4c6f-d003-0314a0dbe87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   UserID  MovieID  Rating                        Title\n",
            "0       1        1     4.0             Toy Story (1995)\n",
            "1       1        3     4.0      Grumpier Old Men (1995)\n",
            "2       1        6     4.0                  Heat (1995)\n",
            "3       1       47     5.0  Seven (a.k.a. Se7en) (1995)\n",
            "4       1       50     5.0   Usual Suspects, The (1995)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "\n",
        "# Read the data from your CSV file\n",
        "df = pd.read_csv(\"/content/fina.csv\")\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Ensure that the DataFrame has the required columns\n",
        "# Columns needed: 'UserID', 'MovieID', 'Rating', 'Title'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "C6Xg57D32TIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Data Preprocessing ------------------- #\n",
        "\n",
        "# Map UserID and MovieID to indices starting from 0\n",
        "user_ids = df['UserID'].unique()\n",
        "movie_ids = df['MovieID'].unique()\n",
        "\n",
        "# Create mappings from original IDs to indices\n",
        "user2idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
        "idx2user = {idx: user_id for user_id, idx in user2idx.items()}\n",
        "\n",
        "movie2idx = {movie_id: idx for idx, movie_id in enumerate(movie_ids)}\n",
        "idx2movie = {idx: movie_id for movie_id, idx in movie2idx.items()}\n",
        "\n",
        "# Add new columns to the DataFrame with mapped indices\n",
        "df['user_idx'] = df['UserID'].map(user2idx)\n",
        "df['movie_idx'] = df['MovieID'].map(movie2idx)\n",
        "\n",
        "# Number of unique users and movies\n",
        "num_users = len(user_ids)\n",
        "num_movies = len(movie_ids)\n",
        "\n",
        "print(f'Number of users: {num_users}')\n",
        "print(f'Number of movies: {num_movies}')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYvrZQLApj3R",
        "outputId": "658fe783-bc8b-44a6-8e53-31d829fa49b4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of users: 610\n",
            "Number of movies: 9724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Implementation (Neural Collaborative Filtering - NCF)"
      ],
      "metadata": {
        "id": "BNheOKWZ2Z8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Model Implementation ------------------- #\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_size = 10  # Size of the latent embedding vectors\n",
        "mlp_layers = [64, 32, 16, 8]  # Sizes of MLP hidden layers\n",
        "learning_rate = 0.001  # Learning rate for optimizer\n",
        "reg = 0.0001  # Regularization parameter\n",
        "num_epochs = 10  # Number of epochs to train\n",
        "K = 10  # Number of items for Precision@K, Recall@K, NDCG@K\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Initialize user and item embeddings for MF part\n",
        "user_embedding_mf = np.random.normal(scale=0.1, size=(num_users, embedding_size))\n",
        "item_embedding_mf = np.random.normal(scale=0.1, size=(num_movies, embedding_size))\n",
        "\n",
        "# Initialize user and item embeddings for MLP part\n",
        "user_embedding_mlp = np.random.normal(scale=0.1, size=(num_users, embedding_size))\n",
        "item_embedding_mlp = np.random.normal(scale=0.1, size=(num_movies, embedding_size))\n",
        "\n",
        "# Initialize MLP weights and biases\n",
        "mlp_weights = []\n",
        "mlp_biases = []\n",
        "\n",
        "input_size = embedding_size * 2  # Because we concatenate user and item embeddings\n",
        "for layer_size in mlp_layers:\n",
        "    weight = np.random.normal(scale=0.1, size=(input_size, layer_size))\n",
        "    bias = np.zeros(layer_size)\n",
        "    mlp_weights.append(weight)\n",
        "    mlp_biases.append(bias)\n",
        "    input_size = layer_size  # Update input size for the next layer\n",
        "\n",
        "# Initialize output layer weights and bias\n",
        "output_weight = np.random.normal(scale=0.1, size=(embedding_size + mlp_layers[-1], 1))\n",
        "output_bias = np.zeros(1)\n",
        "\n",
        "# Activation functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n"
      ],
      "metadata": {
        "id": "xcCdN87O2V2T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Forward and Backward Pass Functions"
      ],
      "metadata": {
        "id": "vcboNs_G2dMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass function\n",
        "def forward(user_idx, item_idx):\n",
        "    # MF part\n",
        "    mf_user_emb = user_embedding_mf[user_idx]  # Shape: (embedding_size,)\n",
        "    mf_item_emb = item_embedding_mf[item_idx]  # Shape: (embedding_size,)\n",
        "    mf_vector = mf_user_emb * mf_item_emb      # Element-wise multiplication\n",
        "\n",
        "    # MLP part\n",
        "    mlp_user_emb = user_embedding_mlp[user_idx]  # Shape: (embedding_size,)\n",
        "    mlp_item_emb = item_embedding_mlp[item_idx]  # Shape: (embedding_size,)\n",
        "    mlp_vector = np.concatenate([mlp_user_emb, mlp_item_emb])  # Shape: (embedding_size * 2,)\n",
        "\n",
        "    activations = [mlp_vector]\n",
        "    pre_activations = []\n",
        "\n",
        "    # Forward pass through MLP layers\n",
        "    for weight, bias in zip(mlp_weights, mlp_biases):\n",
        "        z = np.dot(activations[-1], weight) + bias\n",
        "        pre_activations.append(z)\n",
        "        a = relu(z)\n",
        "        activations.append(a)\n",
        "\n",
        "    # Concatenate MF and MLP parts\n",
        "    final_vector = np.concatenate([mf_vector, activations[-1]])  # Shape: (embedding_size + mlp_layers[-1],)\n",
        "\n",
        "    # Output layer\n",
        "    prediction = np.dot(final_vector, output_weight) + output_bias  # Shape: (1,)\n",
        "\n",
        "    # Cache intermediate values for backpropagation\n",
        "    cache = {\n",
        "        'mf_user_emb': mf_user_emb,\n",
        "        'mf_item_emb': mf_item_emb,\n",
        "        'mlp_user_emb': mlp_user_emb,\n",
        "        'mlp_item_emb': mlp_item_emb,\n",
        "        'activations': activations,\n",
        "        'pre_activations': pre_activations,\n",
        "        'final_vector': final_vector\n",
        "    }\n",
        "\n",
        "    return prediction.flatten()[0], cache\n",
        "\n",
        "# Loss function (Mean Squared Error)\n",
        "def compute_loss(prediction, target):\n",
        "    loss = 0.5 * (prediction - target) ** 2\n",
        "    return loss\n",
        "\n",
        "# Backward pass function\n",
        "def backward(target, prediction, cache, user_idx, item_idx):\n",
        "    global user_embedding_mf, item_embedding_mf\n",
        "    global user_embedding_mlp, item_embedding_mlp\n",
        "    global mlp_weights, mlp_biases, output_weight, output_bias\n",
        "\n",
        "    # Compute gradient of the loss w.r.t. prediction\n",
        "    d_loss_pred = prediction - target  # Derivative of MSE loss w.r.t. prediction\n",
        "\n",
        "    # Gradient for the output layer\n",
        "    d_output_weight = np.outer(cache['final_vector'], d_loss_pred) + reg * output_weight\n",
        "    d_output_bias = d_loss_pred\n",
        "\n",
        "    # Backpropagate to final_vector\n",
        "    d_final_vector = output_weight.flatten() * d_loss_pred\n",
        "\n",
        "    # Split gradients back to MF and MLP parts\n",
        "    mf_size = embedding_size\n",
        "    mlp_size = mlp_layers[-1]\n",
        "\n",
        "    d_mf_vector = d_final_vector[:mf_size]\n",
        "    d_mlp_vector = d_final_vector[mf_size:]\n",
        "\n",
        "    # Gradients for MF embeddings\n",
        "    d_mf_user_emb = d_mf_vector * cache['mf_item_emb'] + reg * cache['mf_user_emb']\n",
        "    d_mf_item_emb = d_mf_vector * cache['mf_user_emb'] + reg * cache['mf_item_emb']\n",
        "\n",
        "    # Update MF embeddings\n",
        "    user_embedding_mf[user_idx] -= learning_rate * d_mf_user_emb\n",
        "    item_embedding_mf[item_idx] -= learning_rate * d_mf_item_emb\n",
        "\n",
        "    # Backpropagate through MLP layers\n",
        "    d_activation = d_mlp_vector\n",
        "    for i in reversed(range(len(mlp_layers))):\n",
        "        # Derivative w.r.t pre-activation\n",
        "        d_pre_activation = d_activation * relu_derivative(cache['pre_activations'][i])\n",
        "\n",
        "        # Gradients for weights and biases\n",
        "        d_weight = np.outer(cache['activations'][i], d_pre_activation) + reg * mlp_weights[i]\n",
        "        d_bias = d_pre_activation\n",
        "\n",
        "        # Gradient w.r.t previous activation\n",
        "        d_activation_prev = np.dot(d_pre_activation, mlp_weights[i].T)\n",
        "\n",
        "        # Update weights and biases\n",
        "        mlp_weights[i] -= learning_rate * d_weight\n",
        "        mlp_biases[i] -= learning_rate * d_bias\n",
        "\n",
        "        # Update activation for next layer\n",
        "        d_activation = d_activation_prev\n",
        "\n",
        "    # Gradients for MLP embeddings\n",
        "    d_mlp_user_emb = d_activation[:embedding_size] + reg * cache['mlp_user_emb']\n",
        "    d_mlp_item_emb = d_activation[embedding_size:] + reg * cache['mlp_item_emb']\n",
        "\n",
        "    # Update MLP embeddings\n",
        "    user_embedding_mlp[user_idx] -= learning_rate * d_mlp_user_emb\n",
        "    item_embedding_mlp[item_idx] -= learning_rate * d_mlp_item_emb\n",
        "\n",
        "    # Update output layer weights and bias\n",
        "    output_weight -= learning_rate * d_output_weight\n",
        "    output_bias -= learning_rate * d_output_bias\n"
      ],
      "metadata": {
        "id": "_plOTdDu2bTv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "cGTBLlny2gIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Training Loop ------------------- #\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    # Shuffle training data\n",
        "    shuffled_indices = np.random.permutation(len(train_df))\n",
        "    for idx in shuffled_indices:\n",
        "        row = train_df.iloc[idx]\n",
        "        user_idx = int(row['user_idx'])\n",
        "        item_idx = int(row['movie_idx'])\n",
        "        rating = row['Rating']\n",
        "\n",
        "        # Forward pass\n",
        "        prediction, cache = forward(user_idx, item_idx)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = compute_loss(prediction, rating)\n",
        "        total_loss += loss\n",
        "\n",
        "        # Backward pass and update weights\n",
        "        backward(rating, prediction, cache, user_idx, item_idx)\n",
        "\n",
        "    avg_loss = total_loss / len(train_df)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVsUZeEY2eIQ",
        "outputId": "f631c633-1608-428b-c970-209fd3018a53"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.5658\n",
            "Epoch 2/10, Loss: 0.4859\n",
            "Epoch 3/10, Loss: 0.4499\n",
            "Epoch 4/10, Loss: 0.4353\n",
            "Epoch 5/10, Loss: 0.4234\n",
            "Epoch 6/10, Loss: 0.4122\n",
            "Epoch 7/10, Loss: 0.4019\n",
            "Epoch 8/10, Loss: 0.3925\n",
            "Epoch 9/10, Loss: 0.3842\n",
            "Epoch 10/10, Loss: 0.3767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Evaluation Metrics"
      ],
      "metadata": {
        "id": "fpoaLM5H2ixv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Evaluation Metrics ------------------- #\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = []\n",
        "test_targets = []\n",
        "\n",
        "for idx in range(len(test_df)):\n",
        "    row = test_df.iloc[idx]\n",
        "    user_idx = int(row['user_idx'])\n",
        "    item_idx = int(row['movie_idx'])\n",
        "    rating = row['Rating']\n",
        "\n",
        "    # Forward pass\n",
        "    prediction, _ = forward(user_idx, item_idx)\n",
        "\n",
        "    test_predictions.append(prediction)\n",
        "    test_targets.append(rating)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "test_predictions = np.array(test_predictions)\n",
        "test_targets = np.array(test_targets)\n",
        "\n",
        "# Compute MAE and RMSE\n",
        "mae = np.mean(np.abs(test_predictions - test_targets))\n",
        "rmse = np.sqrt(np.mean((test_predictions - test_targets) ** 2))\n",
        "\n",
        "print(f'\\nTest MAE: {mae:.4f}')\n",
        "print(f'Test RMSE: {rmse:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lS0L-B52hD5",
        "outputId": "7a0f1ac6-7f0b-44ea-af33-54ad1b2c33ef"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test MAE: 0.6875\n",
            "Test RMSE: 0.9067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute Precision@K, Recall@K, NDCG@K"
      ],
      "metadata": {
        "id": "FRvOLApx2mRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision@K, Recall@K, NDCG@K\n",
        "# Build user-item interactions for test data\n",
        "user_test_items = defaultdict(set)\n",
        "for row in test_df.itertuples():\n",
        "    user_test_items[int(row.user_idx)].add(int(row.movie_idx))\n",
        "\n",
        "# Build user-item interactions for training data\n",
        "user_train_items = defaultdict(set)\n",
        "for row in train_df.itertuples():\n",
        "    user_train_items[int(row.user_idx)].add(int(row.movie_idx))\n",
        "\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "ndcg_list = []\n",
        "\n",
        "# All movie indices\n",
        "all_movie_indices = np.arange(num_movies)\n",
        "\n",
        "for user_idx in range(num_users):\n",
        "    train_items = user_train_items[user_idx]\n",
        "    test_items = user_test_items[user_idx]\n",
        "\n",
        "    if not test_items:\n",
        "        continue  # Skip users with no test data\n",
        "\n",
        "    # Items to predict (exclude items in training data)\n",
        "    items_to_predict = list(set(all_movie_indices) - train_items)\n",
        "    predictions = []\n",
        "\n",
        "    for item_idx in items_to_predict:\n",
        "        pred, _ = forward(user_idx, item_idx)\n",
        "        predictions.append((item_idx, pred))\n",
        "\n",
        "    # Rank items by predicted score\n",
        "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "    ranked_items = [item for item, score in predictions]\n",
        "\n",
        "    # Top K items\n",
        "    top_k_items = ranked_items[:K]\n",
        "\n",
        "    # Compute hits\n",
        "    hits = [1 if item in test_items else 0 for item in top_k_items]\n",
        "\n",
        "    # Precision@K\n",
        "    precision = np.sum(hits) / K\n",
        "    precision_list.append(precision)\n",
        "\n",
        "    # Recall@K\n",
        "    recall = np.sum(hits) / len(test_items)\n",
        "    recall_list.append(recall)\n",
        "\n",
        "    # NDCG@K\n",
        "    dcg = 0\n",
        "    for i, hit in enumerate(hits):\n",
        "        dcg += hit / np.log2(i + 2)  # i+2 because log2(1) = 0\n",
        "    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(test_items), K)))\n",
        "    ndcg = dcg / idcg if idcg > 0 else 0\n",
        "    ndcg_list.append(ndcg)\n",
        "\n",
        "# Compute average metrics\n",
        "avg_precision = np.mean(precision_list)\n",
        "avg_recall = np.mean(recall_list)\n",
        "avg_ndcg = np.mean(ndcg_list)\n",
        "\n",
        "print(f'\\nAverage Precision@{K}: {avg_precision:.4f}')\n",
        "print(f'Average Recall@{K}: {avg_recall:.4f}')\n",
        "print(f'Average NDCG@{K}: {avg_ndcg:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWIscrLX2j3h",
        "outputId": "aedc110f-983d-4b99-9c8b-99c8c7733800"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Precision@10: 0.0467\n",
            "Average Recall@10: 0.0211\n",
            "Average NDCG@10: 0.0542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the Trained Model"
      ],
      "metadata": {
        "id": "U3Tx0CQG2ssf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Save the Trained Model ------------------- #\n",
        "\n",
        "# Save embeddings and weights\n",
        "np.save('user_embedding_mf.npy', user_embedding_mf)\n",
        "np.save('item_embedding_mf.npy', item_embedding_mf)\n",
        "np.save('user_embedding_mlp.npy', user_embedding_mlp)\n",
        "np.save('item_embedding_mlp.npy', item_embedding_mlp)\n",
        "\n",
        "# Save MLP weights and biases\n",
        "np.savez('mlp_weights.npz', *mlp_weights)\n",
        "np.savez('mlp_biases.npz', *mlp_biases)\n",
        "\n",
        "# Save output layer weights and bias\n",
        "np.save('output_weight.npy', output_weight)\n",
        "np.save('output_bias.npy', output_bias)\n",
        "\n",
        "print(\"Model parameters have been saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwPNbzg_2ncw",
        "outputId": "6814c211-3b94-427b-b5fe-9ae55e37e160"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters have been saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load the Trained Model"
      ],
      "metadata": {
        "id": "h3QKFlPW2wHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Using the Trained Model for Prediction ------------------- #\n",
        "\n",
        "# Example: Predict the rating for a specific user and item\n",
        "def predict_rating(user_id, item_id):\n",
        "    user_idx = user2idx[user_id]\n",
        "    item_idx = movie2idx[item_id]\n",
        "    prediction, _ = forward(user_idx, item_idx)\n",
        "    return prediction\n",
        "\n",
        "# Example usage\n",
        "user_id = df['UserID'].iloc[0]  # Replace with desired UserID\n",
        "item_id = df['MovieID'].iloc[0]  # Replace with desired MovieID\n",
        "\n",
        "predicted_rating = predict_rating(user_id, item_id)\n",
        "print(f'Predicted rating for User {user_id} and Item {item_id}: {predicted_rating:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYAQPwza2uAy",
        "outputId": "ee985018-daf0-4eda-bb4a-5937d7dd1f8f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted rating for User 1 and Item 1: 4.6523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference using the trained Neural Collaborative Filtering (NCF)"
      ],
      "metadata": {
        "id": "6_lIU-fP31H2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries and Load Data Mappings"
      ],
      "metadata": {
        "id": "S7XxAHly4DFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the data mappings from your CSV file\n",
        "df = pd.read_csv(\"/content/fina.csv\")\n",
        "\n",
        "# Map UserID and MovieID to indices starting from 0\n",
        "user_ids = df['UserID'].unique()\n",
        "movie_ids = df['MovieID'].unique()\n",
        "\n",
        "# Create mappings from original IDs to indices\n",
        "user2idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
        "idx2user = {idx: user_id for user_id, idx in user2idx.items()}\n",
        "\n",
        "movie2idx = {movie_id: idx for idx, movie_id in enumerate(movie_ids)}\n",
        "idx2movie = {idx: movie_id for movie_id, idx in movie2idx.items()}\n",
        "\n",
        "# Number of unique users and movies\n",
        "num_users = len(user_ids)\n",
        "num_movies = len(movie_ids)\n",
        "\n",
        "print(f'Number of users: {num_users}')\n",
        "print(f'Number of movies: {num_movies}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NK9MgC44CsJ",
        "outputId": "594a7d19-fe5c-45dc-e2ea-71f85da166dd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of users: 610\n",
            "Number of movies: 9724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Trained Model Parameters"
      ],
      "metadata": {
        "id": "wzkV2NKU4KBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Load the Trained Model ------------------- #\n",
        "\n",
        "# Load embeddings and weights\n",
        "user_embedding_mf = np.load('user_embedding_mf.npy')\n",
        "item_embedding_mf = np.load('item_embedding_mf.npy')\n",
        "user_embedding_mlp = np.load('user_embedding_mlp.npy')\n",
        "item_embedding_mlp = np.load('item_embedding_mlp.npy')\n",
        "\n",
        "# Load MLP weights and biases\n",
        "mlp_weights_loaded = np.load('mlp_weights.npz')\n",
        "mlp_weights = [mlp_weights_loaded[key] for key in mlp_weights_loaded.files]\n",
        "\n",
        "mlp_biases_loaded = np.load('mlp_biases.npz')\n",
        "mlp_biases = [mlp_biases_loaded[key] for key in mlp_biases_loaded.files]\n",
        "\n",
        "# Load output layer weights and bias\n",
        "output_weight = np.load('output_weight.npy')\n",
        "output_bias = np.load('output_bias.npy')\n",
        "\n",
        "print(\"Model parameters have been loaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Xyc7eQ4L1p",
        "outputId": "a7fbfbc6-c443-4338-b98b-d961ddebb0e2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters have been loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Activation Function and Forward Pass Function"
      ],
      "metadata": {
        "id": "YukzxzEo3W8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation function\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Forward pass function\n",
        "def forward(user_idx, item_idx):\n",
        "    # MF part\n",
        "    mf_user_emb = user_embedding_mf[user_idx]  # Shape: (embedding_size,)\n",
        "    mf_item_emb = item_embedding_mf[item_idx]  # Shape: (embedding_size,)\n",
        "    mf_vector = mf_user_emb * mf_item_emb      # Element-wise multiplication\n",
        "\n",
        "    # MLP part\n",
        "    mlp_user_emb = user_embedding_mlp[user_idx]  # Shape: (embedding_size,)\n",
        "    mlp_item_emb = item_embedding_mlp[item_idx]  # Shape: (embedding_size,)\n",
        "    mlp_vector = np.concatenate([mlp_user_emb, mlp_item_emb])  # Shape: (embedding_size * 2,)\n",
        "\n",
        "    activations = [mlp_vector]\n",
        "\n",
        "    # Forward pass through MLP layers\n",
        "    for weight, bias in zip(mlp_weights, mlp_biases):\n",
        "        z = np.dot(activations[-1], weight) + bias\n",
        "        a = relu(z)\n",
        "        activations.append(a)\n",
        "\n",
        "    # Concatenate MF and MLP parts\n",
        "    final_vector = np.concatenate([mf_vector, activations[-1]])  # Shape: (embedding_size + mlp_layers[-1],)\n",
        "\n",
        "    # Output layer\n",
        "    prediction = np.dot(final_vector, output_weight) + output_bias  # Shape: (1,)\n",
        "\n",
        "    return prediction.flatten()[0]\n"
      ],
      "metadata": {
        "id": "AZI50UQT4v2E"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Trained Model for Prediction"
      ],
      "metadata": {
        "id": "iLPxNVc0400_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Using the Trained Model for Prediction ------------------- #\n",
        "\n",
        "# Function to predict rating for a given user and item\n",
        "def predict_rating(user_id, item_id):\n",
        "    if user_id not in user2idx or item_id not in movie2idx:\n",
        "        raise ValueError(\"UserID or MovieID not found in the data.\")\n",
        "    user_idx = user2idx[user_id]\n",
        "    item_idx = movie2idx[item_id]\n",
        "    prediction = forward(user_idx, item_idx)\n",
        "    return prediction\n",
        "\n",
        "# Example usage\n",
        "# Replace with desired UserID and MovieID\n",
        "user_id = df['UserID'].iloc[0]  # Example: first user in the dataset\n",
        "item_id = df['MovieID'].iloc[0]  # Example: first item in the dataset\n",
        "\n",
        "predicted_rating = predict_rating(user_id, item_id)\n",
        "print(f'Predicted rating for User {user_id} and Item {item_id}: {predicted_rating:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCcFLVgC4xCJ",
        "outputId": "d94b8b99-91b0-4398-aff2-00567229974d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted rating for User 1 and Item 1: 4.6523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making Recommendations for a User"
      ],
      "metadata": {
        "id": "YgPA2gmG44CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to recommend top N items for a given user\n",
        "def recommend_top_n(user_id, N=10):\n",
        "    if user_id not in user2idx:\n",
        "        raise ValueError(\"UserID not found in the data.\")\n",
        "    user_idx = user2idx[user_id]\n",
        "\n",
        "    # Get all item indices\n",
        "    all_item_indices = np.arange(num_movies)\n",
        "\n",
        "    # Exclude items the user has already interacted with\n",
        "    user_train_items = df[df['user_idx'] == user_idx]['movie_idx'].tolist()\n",
        "\n",
        "    items_to_predict = [item_idx for item_idx in all_item_indices if item_idx not in user_train_items]\n",
        "\n",
        "    predictions = []\n",
        "    for item_idx in items_to_predict:\n",
        "        prediction = forward(user_idx, item_idx)\n",
        "        predictions.append((item_idx, prediction))\n",
        "\n",
        "    # Sort the predictions\n",
        "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get top N items\n",
        "    top_n_items = predictions[:N]\n",
        "\n",
        "    # Map indices back to MovieIDs and Titles\n",
        "    top_n_movie_ids = [idx2movie[item_idx] for item_idx, _ in top_n_items]\n",
        "    top_n_titles = df[df['MovieID'].isin(top_n_movie_ids)]['Title'].unique()\n",
        "\n",
        "    top_n_scores = [score for _, score in top_n_items]\n",
        "\n",
        "    return list(zip(top_n_movie_ids, top_n_titles, top_n_scores))\n",
        "\n",
        "user_id =  81 # Replace with desired UserID\n",
        "top_n_recommendations = recommend_top_n(user_id, N=5)\n",
        "print(f'Top 5 recommendations for User {user_id}:')\n",
        "for movie_id, title, score in top_n_recommendations:\n",
        "    print(f'MovieID: {movie_id}, Title: {title}, Predicted Rating: {score:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "Lr1bnT8c415X",
        "outputId": "6f32d777-a777-444b-f832-5bc975481983"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'user_idx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'user_idx'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-9ef2e634bad1>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0muser_id\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m81\u001b[0m \u001b[0;31m# Replace with desired UserID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtop_n_recommendations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecommend_top_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Top 5 recommendations for User {user_id}:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmovie_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_n_recommendations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-9ef2e634bad1>\u001b[0m in \u001b[0;36mrecommend_top_n\u001b[0;34m(user_id, N)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Exclude items the user has already interacted with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0muser_train_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_idx'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0muser_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'movie_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mitems_to_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem_idx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_item_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muser_train_items\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'user_idx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ------------------- 1. Load Data and Create Mappings ------------------- #\n",
        "\n",
        "# Load the data mappings from your CSV file\n",
        "df = pd.read_csv(\"/content/fina.csv\")\n",
        "\n",
        "# Map UserID and MovieID to indices starting from 0\n",
        "user_ids = df['UserID'].unique()\n",
        "movie_ids = df['MovieID'].unique()\n",
        "\n",
        "# Create mappings from original IDs to indices\n",
        "user2idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
        "idx2user = {idx: user_id for user_id, idx in user2idx.items()}\n",
        "\n",
        "movie2idx = {movie_id: idx for idx, movie_id in enumerate(movie_ids)}\n",
        "idx2movie = {idx: movie_id for movie_id, idx in movie2idx.items()}\n",
        "\n",
        "# Add new columns to the DataFrame with mapped indices\n",
        "df['user_idx'] = df['UserID'].map(user2idx)\n",
        "df['movie_idx'] = df['MovieID'].map(movie2idx)\n",
        "\n",
        "# Number of unique users and movies\n",
        "num_users = len(user_ids)\n",
        "num_movies = len(movie_ids)\n",
        "\n",
        "print(f'Number of users: {num_users}')\n",
        "print(f'Number of movies: {num_movies}')\n",
        "\n",
        "# ------------------- 2. Load the Trained Model ------------------- #\n",
        "\n",
        "# Load embeddings and weights\n",
        "user_embedding_mf = np.load('user_embedding_mf.npy')\n",
        "item_embedding_mf = np.load('item_embedding_mf.npy')\n",
        "user_embedding_mlp = np.load('user_embedding_mlp.npy')\n",
        "item_embedding_mlp = np.load('item_embedding_mlp.npy')\n",
        "\n",
        "# Load MLP weights and biases\n",
        "mlp_weights_loaded = np.load('mlp_weights.npz')\n",
        "mlp_weights = [mlp_weights_loaded[key] for key in mlp_weights_loaded.files]\n",
        "\n",
        "mlp_biases_loaded = np.load('mlp_biases.npz')\n",
        "mlp_biases = [mlp_biases_loaded[key] for key in mlp_biases_loaded.files]\n",
        "\n",
        "# Load output layer weights and bias\n",
        "output_weight = np.load('output_weight.npy')\n",
        "output_bias = np.load('output_bias.npy')\n",
        "\n",
        "print(\"Model parameters have been loaded.\")\n",
        "\n",
        "# ------------------- 3. Define Activation Function and Forward Pass ------------------- #\n",
        "\n",
        "# Activation function\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Forward pass function\n",
        "def forward(user_idx, item_idx):\n",
        "    # MF part\n",
        "    mf_user_emb = user_embedding_mf[user_idx]  # Shape: (embedding_size,)\n",
        "    mf_item_emb = item_embedding_mf[item_idx]  # Shape: (embedding_size,)\n",
        "    mf_vector = mf_user_emb * mf_item_emb      # Element-wise multiplication\n",
        "\n",
        "    # MLP part\n",
        "    mlp_user_emb = user_embedding_mlp[user_idx]  # Shape: (embedding_size,)\n",
        "    mlp_item_emb = item_embedding_mlp[item_idx]  # Shape: (embedding_size,)\n",
        "    mlp_vector = np.concatenate([mlp_user_emb, mlp_item_emb])  # Shape: (embedding_size * 2,)\n",
        "\n",
        "    activations = [mlp_vector]\n",
        "\n",
        "    # Forward pass through MLP layers\n",
        "    for weight, bias in zip(mlp_weights, mlp_biases):\n",
        "        z = np.dot(activations[-1], weight) + bias\n",
        "        a = relu(z)\n",
        "        activations.append(a)\n",
        "\n",
        "    # Concatenate MF and MLP parts\n",
        "    final_vector = np.concatenate([mf_vector, activations[-1]])  # Shape: (embedding_size + mlp_layers[-1],)\n",
        "\n",
        "    # Output layer\n",
        "    prediction = np.dot(final_vector, output_weight) + output_bias  # Shape: (1,)\n",
        "\n",
        "    return prediction.flatten()[0]\n",
        "\n",
        "# ------------------- 4. Define Prediction and Recommendation Functions ------------------- #\n",
        "\n",
        "# Function to predict rating for a given user and item\n",
        "def predict_rating(user_id, item_id):\n",
        "    if user_id not in user2idx or item_id not in movie2idx:\n",
        "        raise ValueError(\"UserID or MovieID not found in the data.\")\n",
        "    user_idx = user2idx[user_id]\n",
        "    item_idx = movie2idx[item_id]\n",
        "    prediction = forward(user_idx, item_idx)\n",
        "    return prediction\n",
        "\n",
        "# Function to recommend top N items for a given user\n",
        "def recommend_top_n(user_id, N=10):\n",
        "    if user_id not in user2idx:\n",
        "        raise ValueError(\"UserID not found in the data.\")\n",
        "    user_idx = user2idx[user_id]\n",
        "\n",
        "    # Get all item indices\n",
        "    all_item_indices = np.arange(num_movies)\n",
        "\n",
        "    # Exclude items the user has already interacted with\n",
        "    user_train_items = df[df['user_idx'] == user_idx]['movie_idx'].tolist()\n",
        "\n",
        "    items_to_predict = [item_idx for item_idx in all_item_indices if item_idx not in user_train_items]\n",
        "\n",
        "    predictions = []\n",
        "    for item_idx in items_to_predict:\n",
        "        prediction = forward(user_idx, item_idx)\n",
        "        predictions.append((item_idx, prediction))\n",
        "\n",
        "    # Sort the predictions\n",
        "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get top N items\n",
        "    top_n_items = predictions[:N]\n",
        "\n",
        "    # Map indices back to MovieIDs and Titles\n",
        "    top_n_movie_ids = [idx2movie[item_idx] for item_idx, _ in top_n_items]\n",
        "    top_n_titles = df[df['MovieID'].isin(top_n_movie_ids)][['MovieID', 'Title']].drop_duplicates().set_index('MovieID').loc[top_n_movie_ids]['Title'].tolist()\n",
        "\n",
        "    top_n_scores = [score for _, score in top_n_items]\n",
        "\n",
        "    return list(zip(top_n_movie_ids, top_n_titles, top_n_scores))\n",
        "\n",
        "# ------------------- 5. Example Usage ------------------- #\n",
        "\n",
        "# # Predict rating for a specific user and item\n",
        "# user_id = df['UserID'].iloc[0]  # Replace with desired UserID\n",
        "# item_id = df['MovieID'].iloc[0]  # Replace with desired MovieID\n",
        "\n",
        "user_id = 45\n",
        "item_id = 34\n",
        "\n",
        "predicted_rating = predict_rating(user_id, item_id)\n",
        "print(f'Predicted rating for User {user_id} and Item {item_id}: {predicted_rating:.4f}')\n",
        "\n",
        "# Generate top N recommendations for a user\n",
        "top_n_recommendations = recommend_top_n(user_id, N=5)\n",
        "print(f'\\nTop 5 recommendations for User {user_id}:')\n",
        "for movie_id, title, score in top_n_recommendations:\n",
        "    print(f'MovieID: {movie_id}, Title: {title}, Predicted Rating: {score:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvLGrwY46Jk",
        "outputId": "76b24841-bad0-4de5-9db0-5543ad2b2a3a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of users: 610\n",
            "Number of movies: 9724\n",
            "Model parameters have been loaded.\n",
            "Predicted rating for User 45 and Item 34: 4.2130\n",
            "\n",
            "Top 5 recommendations for User 45:\n",
            "MovieID: 318, Title: Shawshank Redemption, The (1994), Predicted Rating: 4.9104\n",
            "MovieID: 55391, Title: 10th & Wolf (2006), Predicted Rating: 4.8409\n",
            "MovieID: 1223, Title: Grand Day Out with Wallace and Gromit, A (1989), Predicted Rating: 4.8243\n",
            "MovieID: 7361, Title: Eternal Sunshine of the Spotless Mind (2004), Predicted Rating: 4.8015\n",
            "MovieID: 43333, Title: Water (2005), Predicted Rating: 4.7993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SReH2Zjm6KIM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}