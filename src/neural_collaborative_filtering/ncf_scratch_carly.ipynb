{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "QRfUTxTowugn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KJ251utruXLs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_N = 5\n"
      ],
      "metadata": {
        "id": "sFWq9UkHudWY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement evaluation metrics"
      ],
      "metadata": {
        "id": "WC7aHzQZw1V-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics for regression\n",
        "def calculate_mae(y_true, y_pred):\n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "def calculate_rmse(y_true, y_pred):\n",
        "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "\n",
        "# Metrics for ranking\n",
        "def precision_at_k(y_true, y_pred, k):\n",
        "    \"\"\"\n",
        "    Precision@K: Fraction of relevant items in the top-K predictions.\n",
        "    \"\"\"\n",
        "    top_k_indices = np.argsort(y_pred)[::-1][:k]\n",
        "    relevant_items = y_true[top_k_indices]\n",
        "    return np.sum(relevant_items) / k\n",
        "\n",
        "def recall_at_k(y_true, y_pred, k):\n",
        "    \"\"\"\n",
        "    Recall@K: Fraction of relevant items among all relevant items.\n",
        "    \"\"\"\n",
        "    if np.sum(y_true) == 0:  # No relevant items\n",
        "        return 0.0\n",
        "    top_k_indices = np.argsort(y_pred)[::-1][:k]\n",
        "    relevant_items = y_true[top_k_indices]\n",
        "    return np.sum(relevant_items) / np.sum(y_true)\n",
        "\n",
        "def ndcg_at_k(y_true, y_pred, k):\n",
        "    \"\"\"\n",
        "    NDCG@K: Normalized Discounted Cumulative Gain.\n",
        "    \"\"\"\n",
        "    top_k_indices = np.argsort(y_pred)[::-1][:k]\n",
        "    relevant_items = y_true[top_k_indices]\n",
        "\n",
        "    # DCG: Discounted Cumulative Gain\n",
        "    dcg = np.sum(relevant_items / np.log2(np.arange(2, k + 2)))\n",
        "\n",
        "    # IDCG: Ideal DCG (sorted by relevance)\n",
        "    ideal_relevance = np.sort(y_true)[::-1][:k]\n",
        "    idcg = np.sum(ideal_relevance / np.log2(np.arange(2, k + 2)))\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0"
      ],
      "metadata": {
        "id": "wEnJm0Jbujta"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement NCF dataset and models"
      ],
      "metadata": {
        "id": "mTR-ifWqxCsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset for NCF\n",
        "class NCFDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, user_ids, movie_ids, ratings):\n",
        "        self.user_ids = torch.tensor(user_ids.to_numpy(), dtype=torch.long)\n",
        "        self.movie_ids = torch.tensor(movie_ids.to_numpy(), dtype=torch.long)\n",
        "        self.ratings = torch.tensor(ratings.to_numpy(), dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ratings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.user_ids[idx], self.movie_ids[idx], self.ratings[idx]\n",
        "\n",
        "# Define the NCF model\n",
        "class NCFModel(torch.nn.Module):\n",
        "    def __init__(self, num_users, num_movies, embed_dim):\n",
        "        super(NCFModel, self).__init__()\n",
        "        self.user_embedding = torch.nn.Embedding(num_users, embed_dim)\n",
        "        self.movie_embedding = torch.nn.Embedding(num_movies, embed_dim)\n",
        "        self.fc = torch.nn.Sequential(\n",
        "            torch.nn.Linear(embed_dim * 2, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 1),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, user_ids, movie_ids):\n",
        "        user_embeds = self.user_embedding(user_ids)\n",
        "        movie_embeds = self.movie_embedding(movie_ids)\n",
        "        x = torch.cat([user_embeds, movie_embeds], dim=-1)\n",
        "        return self.fc(x).squeeze() * 4 + 1 # scale to [1,5]\n",
        "\n",
        "# Define the NeuMF model\n",
        "class NeuMFModel(nn.Module):\n",
        "    def __init__(self, num_users, num_movies, embed_dim, mlp_layer_sizes=[16, 8, 4]):\n",
        "        super(NeuMFModel, self).__init__()\n",
        "\n",
        "        # GMF Embeddings\n",
        "        self.gmf_user_embedding = nn.Embedding(num_users, embed_dim)\n",
        "        self.gmf_movie_embedding = nn.Embedding(num_movies, embed_dim)\n",
        "\n",
        "        # MLP Embeddings\n",
        "        self.mlp_user_embedding = nn.Embedding(num_users, mlp_layer_sizes[0] // 2)\n",
        "        self.mlp_movie_embedding = nn.Embedding(num_movies, mlp_layer_sizes[0] // 2)\n",
        "\n",
        "        # MLP Layers\n",
        "        mlp_layers = []\n",
        "        input_size = mlp_layer_sizes[0]\n",
        "        for output_size in mlp_layer_sizes[1:]:\n",
        "            mlp_layers.append(nn.Linear(input_size, output_size))\n",
        "            mlp_layers.append(nn.ReLU())\n",
        "            input_size = output_size\n",
        "        self.mlp = nn.Sequential(*mlp_layers)\n",
        "\n",
        "        # Final Layer\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.Linear(embed_dim + mlp_layer_sizes[-1], 1),  # Combine GMF and MLP outputs\n",
        "            nn.Sigmoid()  # Output between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, user_ids, movie_ids):\n",
        "        # GMF Component\n",
        "        gmf_user_embeds = self.gmf_user_embedding(user_ids)\n",
        "        gmf_movie_embeds = self.gmf_movie_embedding(movie_ids)\n",
        "        gmf_output = gmf_user_embeds * gmf_movie_embeds  # Element-wise product\n",
        "\n",
        "        # MLP Component\n",
        "        mlp_user_embeds = self.mlp_user_embedding(user_ids)\n",
        "        mlp_movie_embeds = self.mlp_movie_embedding(movie_ids)\n",
        "        mlp_input = torch.cat([mlp_user_embeds, mlp_movie_embeds], dim=-1)\n",
        "        mlp_output = self.mlp(mlp_input)\n",
        "\n",
        "        # Combine GMF and MLP\n",
        "        combined = torch.cat([gmf_output, mlp_output], dim=-1)\n",
        "\n",
        "        # Final Prediction\n",
        "        output = self.final_layer(combined)\n",
        "\n",
        "        # Scale output to range [1, 5]\n",
        "        return output.squeeze() * 4 + 1\n"
      ],
      "metadata": {
        "id": "h3e4GD5kukYP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define methods to train and evaluate the model"
      ],
      "metadata": {
        "id": "kLxz1ywSxIDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "def train_model(model, data_loader, criterion, optimizer, epochs=5, k=5):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for user_ids, movie_ids, ratings in data_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(user_ids, movie_ids)\n",
        "            loss = criterion(outputs, ratings)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1} | Loss: {total_loss / len(data_loader):.4f}\")\n",
        "\n",
        "# Validation with metrics\n",
        "def evaluate_model(model, data_loader, k=TOP_N):\n",
        "    model.eval()\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for user_ids, movie_ids, ratings in data_loader:\n",
        "            outputs = model(user_ids, movie_ids)\n",
        "            all_targets.extend(ratings.cpu().numpy())\n",
        "            all_predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    all_targets = np.array(all_targets)\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    mae = calculate_mae(all_targets, all_predictions)\n",
        "    rmse = calculate_rmse(all_targets, all_predictions)\n",
        "\n",
        "    binary_targets = (all_targets > 4.5).astype(int)  # Example threshold: 4.5\n",
        "    precision = precision_at_k(binary_targets, all_predictions, k)\n",
        "    recall = recall_at_k(binary_targets, all_predictions, k)\n",
        "    ndcg = ndcg_at_k(binary_targets, all_predictions, k)\n",
        "\n",
        "    print(f\"Evaluation | MAE: {mae:.4f} | RMSE: {rmse:.4f} | Precision@{k}: {precision:.4f} | \"\n",
        "          f\"Recall@{k}: {recall:.4f} | NDCG@{k}: {ndcg:.4f}\")"
      ],
      "metadata": {
        "id": "qwCnQpseuoha"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset, split the train set and test set"
      ],
      "metadata": {
        "id": "SInSsvb_xMQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('Final_data.csv')\n",
        "# Preprocessing: Convert UserID and MovieID to categorical codes\n",
        "data['UserID'] = data['UserID'].astype('category').cat.codes\n",
        "data['MovieID'] = data['MovieID'].astype('category').cat.codes\n",
        "\n",
        "# Extract features for NCF\n",
        "user_ids = data['UserID']\n",
        "movie_ids = data['MovieID']\n",
        "ratings = data['Rating']\n",
        "\n",
        "# Determine unique users and movies for embedding\n",
        "num_users = user_ids.nunique()\n",
        "num_movies = movie_ids.nunique()\n",
        "embed_dim = 50\n",
        "\n",
        "# Train-test split\n",
        "user_train, user_test, movie_train, movie_test, rating_train, rating_test = train_test_split(\n",
        "    user_ids, movie_ids, ratings, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = NCFDataset(user_train, movie_train, rating_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "test_dataset = NCFDataset(user_test, movie_test, rating_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
      ],
      "metadata": {
        "id": "nFKCV51JuuUe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model and evaluate it"
      ],
      "metadata": {
        "id": "73SkAt2bxQk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the model\n",
        "model = NCFModel(num_users, num_movies, embed_dim)\n",
        "# model = NeuMFModel(num_users, num_movies, embed_dim)\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "train_model(model, train_loader, criterion, optimizer, epochs=100, k=5)\n",
        "evaluate_model(model, test_loader, k=TOP_N)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_CTM1MJu0Fs",
        "outputId": "8c13de13-97ca-47a8-e216-2d969d6805e7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 0.9695\n",
            "Epoch 2 | Loss: 0.8464\n",
            "Epoch 3 | Loss: 0.7805\n",
            "Epoch 4 | Loss: 0.7287\n",
            "Epoch 5 | Loss: 0.6844\n",
            "Epoch 6 | Loss: 0.6455\n",
            "Epoch 7 | Loss: 0.6100\n",
            "Epoch 8 | Loss: 0.5762\n",
            "Epoch 9 | Loss: 0.5453\n",
            "Epoch 10 | Loss: 0.5170\n",
            "Epoch 11 | Loss: 0.4893\n",
            "Epoch 12 | Loss: 0.4600\n",
            "Epoch 13 | Loss: 0.4330\n",
            "Epoch 14 | Loss: 0.4106\n",
            "Epoch 15 | Loss: 0.3866\n",
            "Epoch 16 | Loss: 0.3635\n",
            "Epoch 17 | Loss: 0.3447\n",
            "Epoch 18 | Loss: 0.3238\n",
            "Epoch 19 | Loss: 0.3081\n",
            "Epoch 20 | Loss: 0.2926\n",
            "Epoch 21 | Loss: 0.2760\n",
            "Epoch 22 | Loss: 0.2619\n",
            "Epoch 23 | Loss: 0.2512\n",
            "Epoch 24 | Loss: 0.2372\n",
            "Epoch 25 | Loss: 0.2274\n",
            "Epoch 26 | Loss: 0.2155\n",
            "Epoch 27 | Loss: 0.2065\n",
            "Epoch 28 | Loss: 0.1964\n",
            "Epoch 29 | Loss: 0.1898\n",
            "Epoch 30 | Loss: 0.1814\n",
            "Epoch 31 | Loss: 0.1745\n",
            "Epoch 32 | Loss: 0.1675\n",
            "Epoch 33 | Loss: 0.1619\n",
            "Epoch 34 | Loss: 0.1577\n",
            "Epoch 35 | Loss: 0.1506\n",
            "Epoch 36 | Loss: 0.1456\n",
            "Epoch 37 | Loss: 0.1396\n",
            "Epoch 38 | Loss: 0.1355\n",
            "Epoch 39 | Loss: 0.1318\n",
            "Epoch 40 | Loss: 0.1269\n",
            "Epoch 41 | Loss: 0.1229\n",
            "Epoch 42 | Loss: 0.1211\n",
            "Epoch 43 | Loss: 0.1169\n",
            "Epoch 44 | Loss: 0.1132\n",
            "Epoch 45 | Loss: 0.1118\n",
            "Epoch 46 | Loss: 0.1071\n",
            "Epoch 47 | Loss: 0.1053\n",
            "Epoch 48 | Loss: 0.1028\n",
            "Epoch 49 | Loss: 0.1000\n",
            "Epoch 50 | Loss: 0.0979\n",
            "Epoch 51 | Loss: 0.0948\n",
            "Epoch 52 | Loss: 0.0927\n",
            "Epoch 53 | Loss: 0.0908\n",
            "Epoch 54 | Loss: 0.0879\n",
            "Epoch 55 | Loss: 0.0878\n",
            "Epoch 56 | Loss: 0.0860\n",
            "Epoch 57 | Loss: 0.0838\n",
            "Epoch 58 | Loss: 0.0820\n",
            "Epoch 59 | Loss: 0.0807\n",
            "Epoch 60 | Loss: 0.0781\n",
            "Epoch 61 | Loss: 0.0774\n",
            "Epoch 62 | Loss: 0.0770\n",
            "Epoch 63 | Loss: 0.0751\n",
            "Epoch 64 | Loss: 0.0733\n",
            "Epoch 65 | Loss: 0.0724\n",
            "Epoch 66 | Loss: 0.0717\n",
            "Epoch 67 | Loss: 0.0697\n",
            "Epoch 68 | Loss: 0.0680\n",
            "Epoch 69 | Loss: 0.0677\n",
            "Epoch 70 | Loss: 0.0674\n",
            "Epoch 71 | Loss: 0.0676\n",
            "Epoch 72 | Loss: 0.0651\n",
            "Epoch 73 | Loss: 0.0646\n",
            "Epoch 74 | Loss: 0.0634\n",
            "Epoch 75 | Loss: 0.0628\n",
            "Epoch 76 | Loss: 0.0606\n",
            "Epoch 77 | Loss: 0.0604\n",
            "Epoch 78 | Loss: 0.0604\n",
            "Epoch 79 | Loss: 0.0592\n",
            "Epoch 80 | Loss: 0.0587\n",
            "Epoch 81 | Loss: 0.0574\n",
            "Epoch 82 | Loss: 0.0570\n",
            "Epoch 83 | Loss: 0.0561\n",
            "Epoch 84 | Loss: 0.0568\n",
            "Epoch 85 | Loss: 0.0564\n",
            "Epoch 86 | Loss: 0.0552\n",
            "Epoch 87 | Loss: 0.0548\n",
            "Epoch 88 | Loss: 0.0530\n",
            "Epoch 89 | Loss: 0.0533\n",
            "Epoch 90 | Loss: 0.0528\n",
            "Epoch 91 | Loss: 0.0517\n",
            "Epoch 92 | Loss: 0.0508\n",
            "Epoch 93 | Loss: 0.0498\n",
            "Epoch 94 | Loss: 0.0502\n",
            "Epoch 95 | Loss: 0.0500\n",
            "Epoch 96 | Loss: 0.0506\n",
            "Epoch 97 | Loss: 0.0487\n",
            "Epoch 98 | Loss: 0.0485\n",
            "Epoch 99 | Loss: 0.0474\n",
            "Epoch 100 | Loss: 0.0469\n",
            "Evaluation | MAE: 0.9201 | RMSE: 1.1885 | Precision@5: 0.8000 | Recall@5: 0.0015 | NDCG@5: 0.8688\n"
          ]
        }
      ]
    }
  ]
}