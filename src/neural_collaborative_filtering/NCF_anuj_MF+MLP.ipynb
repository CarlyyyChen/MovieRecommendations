{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux3QeCzQ2QCX"
   },
   "source": [
    "Import Libraries and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fsp31lsDovHR",
    "outputId": "2bf470ef-1746-4c6f-d003-0314a0dbe87f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserID  MovieID  Rating                        Title\n",
      "0       1        1     4.0             Toy Story (1995)\n",
      "1       1        3     4.0      Grumpier Old Men (1995)\n",
      "2       1        6     4.0                  Heat (1995)\n",
      "3       1       47     5.0  Seven (a.k.a. Se7en) (1995)\n",
      "4       1       50     5.0   Usual Suspects, The (1995)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "# Read the data from your CSV file\n",
    "df = pd.read_csv(\"/content/fina.csv\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Ensure that the DataFrame has the required columns\n",
    "# Columns needed: 'UserID', 'MovieID', 'Rating', 'Title'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6Xg57D32TIv"
   },
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYvrZQLApj3R",
    "outputId": "658fe783-bc8b-44a6-8e53-31d829fa49b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 610\n",
      "Number of movies: 9724\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Data Preprocessing ------------------- #\n",
    "\n",
    "# Map UserID and MovieID to indices starting from 0\n",
    "user_ids = df['UserID'].unique()\n",
    "movie_ids = df['MovieID'].unique()\n",
    "\n",
    "# Create mappings from original IDs to indices\n",
    "user2idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "idx2user = {idx: user_id for user_id, idx in user2idx.items()}\n",
    "\n",
    "movie2idx = {movie_id: idx for idx, movie_id in enumerate(movie_ids)}\n",
    "idx2movie = {idx: movie_id for movie_id, idx in movie2idx.items()}\n",
    "\n",
    "# Add new columns to the DataFrame with mapped indices\n",
    "df['user_idx'] = df['UserID'].map(user2idx)\n",
    "df['movie_idx'] = df['MovieID'].map(movie2idx)\n",
    "\n",
    "# Number of unique users and movies\n",
    "num_users = len(user_ids)\n",
    "num_movies = len(movie_ids)\n",
    "\n",
    "print(f'Number of users: {num_users}')\n",
    "print(f'Number of movies: {num_movies}')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNheOKWZ2Z8_"
   },
   "source": [
    "Model Implementation (Neural Collaborative Filtering - NCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xcCdN87O2V2T"
   },
   "outputs": [],
   "source": [
    "# ------------------- Model Implementation ------------------- #\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_size = 10  # Size of the latent embedding vectors\n",
    "mlp_layers = [64, 32, 16, 8]  # Sizes of MLP hidden layers\n",
    "learning_rate = 0.001  # Learning rate for optimizer\n",
    "reg = 0.0001  # Regularization parameter\n",
    "num_epochs = 10  # Number of epochs to train\n",
    "K = 10  # Number of items for Precision@K, Recall@K, NDCG@K\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize user and item embeddings for MF part\n",
    "user_embedding_mf = np.random.normal(scale=0.1, size=(num_users, embedding_size))\n",
    "item_embedding_mf = np.random.normal(scale=0.1, size=(num_movies, embedding_size))\n",
    "\n",
    "# Initialize user and item embeddings for MLP part\n",
    "user_embedding_mlp = np.random.normal(scale=0.1, size=(num_users, embedding_size))\n",
    "item_embedding_mlp = np.random.normal(scale=0.1, size=(num_movies, embedding_size))\n",
    "\n",
    "# Initialize MLP weights and biases\n",
    "mlp_weights = []\n",
    "mlp_biases = []\n",
    "\n",
    "input_size = embedding_size * 2  # Because we concatenate user and item embeddings\n",
    "for layer_size in mlp_layers:\n",
    "    weight = np.random.normal(scale=0.1, size=(input_size, layer_size))\n",
    "    bias = np.zeros(layer_size)\n",
    "    mlp_weights.append(weight)\n",
    "    mlp_biases.append(bias)\n",
    "    input_size = layer_size  # Update input size for the next layer\n",
    "\n",
    "# Initialize output layer weights and bias\n",
    "output_weight = np.random.normal(scale=0.1, size=(embedding_size + mlp_layers[-1], 1))\n",
    "output_bias = np.zeros(1)\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcboNs_G2dMX"
   },
   "source": [
    "Define Forward and Backward Pass Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_plOTdDu2bTv"
   },
   "outputs": [],
   "source": [
    "# Forward pass function\n",
    "def forward(user_idx, item_idx):\n",
    "    # MF part\n",
    "    mf_user_emb = user_embedding_mf[user_idx]  # Shape: (embedding_size,)\n",
    "    mf_item_emb = item_embedding_mf[item_idx]  # Shape: (embedding_size,)\n",
    "    mf_vector = mf_user_emb * mf_item_emb      # Element-wise multiplication\n",
    "\n",
    "    # MLP part\n",
    "    mlp_user_emb = user_embedding_mlp[user_idx]  # Shape: (embedding_size,)\n",
    "    mlp_item_emb = item_embedding_mlp[item_idx]  # Shape: (embedding_size,)\n",
    "    mlp_vector = np.concatenate([mlp_user_emb, mlp_item_emb])  # Shape: (embedding_size * 2,)\n",
    "\n",
    "    activations = [mlp_vector]\n",
    "    pre_activations = []\n",
    "\n",
    "    # Forward pass through MLP layers\n",
    "    for weight, bias in zip(mlp_weights, mlp_biases):\n",
    "        z = np.dot(activations[-1], weight) + bias\n",
    "        pre_activations.append(z)\n",
    "        a = relu(z)\n",
    "        activations.append(a)\n",
    "\n",
    "    # Concatenate MF and MLP parts\n",
    "    final_vector = np.concatenate([mf_vector, activations[-1]])  # Shape: (embedding_size + mlp_layers[-1],)\n",
    "\n",
    "    # Output layer\n",
    "    prediction = np.dot(final_vector, output_weight) + output_bias  # Shape: (1,)\n",
    "\n",
    "    # Cache intermediate values for backpropagation\n",
    "    cache = {\n",
    "        'mf_user_emb': mf_user_emb,\n",
    "        'mf_item_emb': mf_item_emb,\n",
    "        'mlp_user_emb': mlp_user_emb,\n",
    "        'mlp_item_emb': mlp_item_emb,\n",
    "        'activations': activations,\n",
    "        'pre_activations': pre_activations,\n",
    "        'final_vector': final_vector\n",
    "    }\n",
    "\n",
    "    return prediction.flatten()[0], cache\n",
    "\n",
    "# Loss function (Mean Squared Error)\n",
    "def compute_loss(prediction, target):\n",
    "    loss = 0.5 * (prediction - target) ** 2\n",
    "    return loss\n",
    "\n",
    "# Backward pass function\n",
    "def backward(target, prediction, cache, user_idx, item_idx):\n",
    "    global user_embedding_mf, item_embedding_mf\n",
    "    global user_embedding_mlp, item_embedding_mlp\n",
    "    global mlp_weights, mlp_biases, output_weight, output_bias\n",
    "\n",
    "    # Compute gradient of the loss w.r.t. prediction\n",
    "    d_loss_pred = prediction - target  # Derivative of MSE loss w.r.t. prediction\n",
    "\n",
    "    # Gradient for the output layer\n",
    "    d_output_weight = np.outer(cache['final_vector'], d_loss_pred) + reg * output_weight\n",
    "    d_output_bias = d_loss_pred\n",
    "\n",
    "    # Backpropagate to final_vector\n",
    "    d_final_vector = output_weight.flatten() * d_loss_pred\n",
    "\n",
    "    # Split gradients back to MF and MLP parts\n",
    "    mf_size = embedding_size\n",
    "    mlp_size = mlp_layers[-1]\n",
    "\n",
    "    d_mf_vector = d_final_vector[:mf_size]\n",
    "    d_mlp_vector = d_final_vector[mf_size:]\n",
    "\n",
    "    # Gradients for MF embeddings\n",
    "    d_mf_user_emb = d_mf_vector * cache['mf_item_emb'] + reg * cache['mf_user_emb']\n",
    "    d_mf_item_emb = d_mf_vector * cache['mf_user_emb'] + reg * cache['mf_item_emb']\n",
    "\n",
    "    # Update MF embeddings\n",
    "    user_embedding_mf[user_idx] -= learning_rate * d_mf_user_emb\n",
    "    item_embedding_mf[item_idx] -= learning_rate * d_mf_item_emb\n",
    "\n",
    "    # Backpropagate through MLP layers\n",
    "    d_activation = d_mlp_vector\n",
    "    for i in reversed(range(len(mlp_layers))):\n",
    "        # Derivative w.r.t pre-activation\n",
    "        d_pre_activation = d_activation * relu_derivative(cache['pre_activations'][i])\n",
    "\n",
    "        # Gradients for weights and biases\n",
    "        d_weight = np.outer(cache['activations'][i], d_pre_activation) + reg * mlp_weights[i]\n",
    "        d_bias = d_pre_activation\n",
    "\n",
    "        # Gradient w.r.t previous activation\n",
    "        d_activation_prev = np.dot(d_pre_activation, mlp_weights[i].T)\n",
    "\n",
    "        # Update weights and biases\n",
    "        mlp_weights[i] -= learning_rate * d_weight\n",
    "        mlp_biases[i] -= learning_rate * d_bias\n",
    "\n",
    "        # Update activation for next layer\n",
    "        d_activation = d_activation_prev\n",
    "\n",
    "    # Gradients for MLP embeddings\n",
    "    d_mlp_user_emb = d_activation[:embedding_size] + reg * cache['mlp_user_emb']\n",
    "    d_mlp_item_emb = d_activation[embedding_size:] + reg * cache['mlp_item_emb']\n",
    "\n",
    "    # Update MLP embeddings\n",
    "    user_embedding_mlp[user_idx] -= learning_rate * d_mlp_user_emb\n",
    "    item_embedding_mlp[item_idx] -= learning_rate * d_mlp_item_emb\n",
    "\n",
    "    # Update output layer weights and bias\n",
    "    output_weight -= learning_rate * d_output_weight\n",
    "    output_bias -= learning_rate * d_output_bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGTBLlny2gIM"
   },
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVsUZeEY2eIQ",
    "outputId": "f631c633-1608-428b-c970-209fd3018a53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5658\n",
      "Epoch 2/10, Loss: 0.4859\n",
      "Epoch 3/10, Loss: 0.4499\n",
      "Epoch 4/10, Loss: 0.4353\n",
      "Epoch 5/10, Loss: 0.4234\n",
      "Epoch 6/10, Loss: 0.4122\n",
      "Epoch 7/10, Loss: 0.4019\n",
      "Epoch 8/10, Loss: 0.3925\n",
      "Epoch 9/10, Loss: 0.3842\n",
      "Epoch 10/10, Loss: 0.3767\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Training Loop ------------------- #\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    # Shuffle training data\n",
    "    shuffled_indices = np.random.permutation(len(train_df))\n",
    "    for idx in shuffled_indices:\n",
    "        row = train_df.iloc[idx]\n",
    "        user_idx = int(row['user_idx'])\n",
    "        item_idx = int(row['movie_idx'])\n",
    "        rating = row['Rating']\n",
    "\n",
    "        # Forward pass\n",
    "        prediction, cache = forward(user_idx, item_idx)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = compute_loss(prediction, rating)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backward pass and update weights\n",
    "        backward(rating, prediction, cache, user_idx, item_idx)\n",
    "\n",
    "    avg_loss = total_loss / len(train_df)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpoaLM5H2ixv"
   },
   "source": [
    " Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6lS0L-B52hD5",
    "outputId": "7a0f1ac6-7f0b-44ea-af33-54ad1b2c33ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test MAE: 0.6875\n",
      "Test RMSE: 0.9067\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Evaluation Metrics ------------------- #\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "\n",
    "for idx in range(len(test_df)):\n",
    "    row = test_df.iloc[idx]\n",
    "    user_idx = int(row['user_idx'])\n",
    "    item_idx = int(row['movie_idx'])\n",
    "    rating = row['Rating']\n",
    "\n",
    "    # Forward pass\n",
    "    prediction, _ = forward(user_idx, item_idx)\n",
    "\n",
    "    test_predictions.append(prediction)\n",
    "    test_targets.append(rating)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_targets = np.array(test_targets)\n",
    "\n",
    "# Compute MAE and RMSE\n",
    "mae = np.mean(np.abs(test_predictions - test_targets))\n",
    "rmse = np.sqrt(np.mean((test_predictions - test_targets) ** 2))\n",
    "\n",
    "print(f'\\nTest MAE: {mae:.4f}')\n",
    "print(f'Test RMSE: {rmse:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRvOLApx2mRp"
   },
   "source": [
    "Compute Precision@K, Recall@K, NDCG@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWIscrLX2j3h",
    "outputId": "aedc110f-983d-4b99-9c8b-99c8c7733800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Precision@10: 0.0467\n",
      "Average Recall@10: 0.0211\n",
      "Average NDCG@10: 0.0542\n"
     ]
    }
   ],
   "source": [
    "# Precision@K, Recall@K, NDCG@K\n",
    "# Build user-item interactions for test data\n",
    "user_test_items = defaultdict(set)\n",
    "for row in test_df.itertuples():\n",
    "    user_test_items[int(row.user_idx)].add(int(row.movie_idx))\n",
    "\n",
    "# Build user-item interactions for training data\n",
    "user_train_items = defaultdict(set)\n",
    "for row in train_df.itertuples():\n",
    "    user_train_items[int(row.user_idx)].add(int(row.movie_idx))\n",
    "\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "ndcg_list = []\n",
    "\n",
    "# All movie indices\n",
    "all_movie_indices = np.arange(num_movies)\n",
    "\n",
    "for user_idx in range(num_users):\n",
    "    train_items = user_train_items[user_idx]\n",
    "    test_items = user_test_items[user_idx]\n",
    "\n",
    "    if not test_items:\n",
    "        continue  # Skip users with no test data\n",
    "\n",
    "    # Items to predict (exclude items in training data)\n",
    "    items_to_predict = list(set(all_movie_indices) - train_items)\n",
    "    predictions = []\n",
    "\n",
    "    for item_idx in items_to_predict:\n",
    "        pred, _ = forward(user_idx, item_idx)\n",
    "        predictions.append((item_idx, pred))\n",
    "\n",
    "    # Rank items by predicted score\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    ranked_items = [item for item, score in predictions]\n",
    "\n",
    "    # Top K items\n",
    "    top_k_items = ranked_items[:K]\n",
    "\n",
    "    # Compute hits\n",
    "    hits = [1 if item in test_items else 0 for item in top_k_items]\n",
    "\n",
    "    # Precision@K\n",
    "    precision = np.sum(hits) / K\n",
    "    precision_list.append(precision)\n",
    "\n",
    "    # Recall@K\n",
    "    recall = np.sum(hits) / len(test_items)\n",
    "    recall_list.append(recall)\n",
    "\n",
    "    # NDCG@K\n",
    "    dcg = 0\n",
    "    for i, hit in enumerate(hits):\n",
    "        dcg += hit / np.log2(i + 2)  # i+2 because log2(1) = 0\n",
    "    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(test_items), K)))\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "    ndcg_list.append(ndcg)\n",
    "\n",
    "# Compute average metrics\n",
    "avg_precision = np.mean(precision_list)\n",
    "avg_recall = np.mean(recall_list)\n",
    "avg_ndcg = np.mean(ndcg_list)\n",
    "\n",
    "print(f'\\nAverage Precision@{K}: {avg_precision:.4f}')\n",
    "print(f'Average Recall@{K}: {avg_recall:.4f}')\n",
    "print(f'Average NDCG@{K}: {avg_ndcg:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3Tx0CQG2ssf"
   },
   "source": [
    "Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwPNbzg_2ncw",
    "outputId": "6814c211-3b94-427b-b5fe-9ae55e37e160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters have been saved.\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Save the Trained Model ------------------- #\n",
    "\n",
    "# Save embeddings and weights\n",
    "np.save('user_embedding_mf.npy', user_embedding_mf)\n",
    "np.save('item_embedding_mf.npy', item_embedding_mf)\n",
    "np.save('user_embedding_mlp.npy', user_embedding_mlp)\n",
    "np.save('item_embedding_mlp.npy', item_embedding_mlp)\n",
    "\n",
    "# Save MLP weights and biases\n",
    "np.savez('mlp_weights.npz', *mlp_weights)\n",
    "np.savez('mlp_biases.npz', *mlp_biases)\n",
    "\n",
    "# Save output layer weights and bias\n",
    "np.save('output_weight.npy', output_weight)\n",
    "np.save('output_bias.npy', output_bias)\n",
    "\n",
    "print(\"Model parameters have been saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
