{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Matrix Factorization using Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anuj Patel(002874710)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Load and Preview the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preview:\n",
      "   UserID  MovieID  Rating                 Title\n",
      "0       1      122     5.0      Boomerang (1992)\n",
      "1       1      185     5.0       Net, The (1995)\n",
      "2       1      231     5.0  Dumb & Dumber (1994)\n",
      "3       1      292     5.0       Outbreak (1995)\n",
      "4       1      316     5.0       Stargate (1994)\n",
      "\n",
      "Dataset shape: (10000054, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:/Users/anujp/OneDrive/Desktop/MovieRecommendations/data/Final_data/Final_data.csv\")\n",
    "\n",
    "# Preview the first few rows\n",
    "print(\"Data preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check the shape of the dataset\n",
    "print(\"\\nDataset shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preview:\n",
      "   UserID  MovieID  Rating                 Title\n",
      "0       1      122     5.0      Boomerang (1992)\n",
      "1       1      185     5.0       Net, The (1995)\n",
      "2       1      231     5.0  Dumb & Dumber (1994)\n",
      "3       1      292     5.0       Outbreak (1995)\n",
      "4       1      316     5.0       Stargate (1994)\n",
      "\n",
      "Dataset shape: (10000054, 4)\n",
      "\n",
      "Dataset Information:\n",
      "-------------------\n",
      "Number of unique users: 69878\n",
      "Number of unique movies: 10677\n",
      "Rating statistics:\n",
      "count    1.000005e+07\n",
      "mean     3.512422e+00\n",
      "std      1.060418e+00\n",
      "min      5.000000e-01\n",
      "25%      3.000000e+00\n",
      "50%      4.000000e+00\n",
      "75%      4.000000e+00\n",
      "max      5.000000e+00\n",
      "Name: Rating, dtype: float64\n",
      "\n",
      "Missing values:\n",
      "UserID     0\n",
      "MovieID    0\n",
      "Rating     0\n",
      "Title      0\n",
      "dtype: int64\n",
      "\n",
      "Rating distribution:\n",
      "Rating\n",
      "0.5      94988\n",
      "1.0     384180\n",
      "1.5     118278\n",
      "2.0     790306\n",
      "2.5     370178\n",
      "3.0    2356676\n",
      "3.5     879764\n",
      "4.0    2875850\n",
      "4.5     585022\n",
      "5.0    1544812\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data types:\n",
      "UserID       int64\n",
      "MovieID      int64\n",
      "Rating     float64\n",
      "Title       object\n",
      "dtype: object\n",
      "\n",
      "Value ranges:\n",
      "UserID range: 1 to 71567\n",
      "MovieID range: 1 to 65133\n",
      "Rating range: 0.5 to 5.0\n"
     ]
    }
   ],
   "source": [
    "# Load and examine the dataset\n",
    "df = pd.read_csv(\"C:/Users/anujp/OneDrive/Desktop/MovieRecommendations/data/Final_data/Final_data.csv\")\n",
    "\n",
    "# Basic information\n",
    "print(\"Data preview:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset shape:\", df.shape)\n",
    "\n",
    "# Additional data examination\n",
    "print(\"\\nDataset Information:\")\n",
    "print(\"-------------------\")\n",
    "print(\"Number of unique users:\", df['UserID'].nunique())\n",
    "print(\"Number of unique movies:\", df['MovieID'].nunique())\n",
    "print(\"Rating statistics:\")\n",
    "print(df['Rating'].describe())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nRating distribution:\")\n",
    "print(df['Rating'].value_counts().sort_index())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check value ranges\n",
    "print(\"\\nValue ranges:\")\n",
    "print(\"UserID range:\", df['UserID'].min(), \"to\", df['UserID'].max())\n",
    "print(\"MovieID range:\", df['MovieID'].min(), \"to\", df['MovieID'].max())\n",
    "print(\"Rating range:\", df['Rating'].min(), \"to\", df['Rating'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation - Map User and Movie IDs to Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mapping IDs to indices...\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Map IDs to Indices\n",
    "print(\"\\nMapping IDs to indices...\")\n",
    "user_ids = df['UserID'].unique()\n",
    "movie_ids = df['MovieID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create forward mappings\n",
    "user2idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "movie2idx = {movie_id: idx for idx, movie_id in enumerate(movie_ids)}\n",
    "\n",
    "# Create reverse mappings\n",
    "idx2user = {idx: user_id for user_id, idx in user2idx.items()}\n",
    "idx2movie = {idx: movie_id for movie_id, idx in movie2idx.items()}\n",
    "\n",
    "# Map indices to dataframe\n",
    "df['user_idx'] = df['UserID'].map(user2idx)\n",
    "df['movie_idx'] = df['MovieID'].map(movie2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(\"\\nSplitting data...\")\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure All Users and Movies in Test Set are in Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering test set...\n",
      "Number of users in training set: 69878\n",
      "Number of movies in training set: 10653\n",
      "Number of users in test set: 69796\n",
      "Number of movies in test set: 10194\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFiltering test set...\")\n",
    "train_users = set(train_data['user_idx'])\n",
    "train_movies = set(train_data['movie_idx'])\n",
    "test_data = test_data[test_data['user_idx'].isin(train_users) & test_data['movie_idx'].isin(train_movies)]\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "print(\"Number of users in training set:\", len(train_users))\n",
    "print(\"Number of movies in training set:\", len(train_movies))\n",
    "print(\"Number of users in test set:\", test_data['user_idx'].nunique())\n",
    "print(\"Number of movies in test set:\", test_data['movie_idx'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Latent Factor Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing matrices...\n",
      "User latent factor matrix shape: (69878, 20)\n",
      "Item latent factor matrix shape: (10677, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInitializing matrices...\")\n",
    "num_users = len(user_ids)\n",
    "num_movies = len(movie_ids)\n",
    "num_factors = 20\n",
    "\n",
    "np.random.seed(42)\n",
    "P = np.random.normal(scale=0.1, size=(num_users, num_factors))\n",
    "Q = np.random.normal(scale=0.1, size=(num_movies, num_factors))\n",
    "\n",
    "print(\"User latent factor matrix shape:\", P.shape)\n",
    "print(\"Item latent factor matrix shape:\", Q.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Functions for RMSE and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Define Functions\n",
    "def validate_input_data(train_data, test_data, P, Q):\n",
    "    \"\"\"Validate input data and matrix dimensions\"\"\"\n",
    "    if not isinstance(train_data, pd.DataFrame) or not isinstance(test_data, pd.DataFrame):\n",
    "        raise TypeError(\"Train and test data must be pandas DataFrames\")\n",
    "    \n",
    "    required_cols = ['user_idx', 'movie_idx', 'Rating']\n",
    "    for col in required_cols:\n",
    "        if col not in train_data.columns or col not in test_data.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    \n",
    "    if train_data['Rating'].min() < 0 or test_data['Rating'].min() < 0:\n",
    "        raise ValueError(\"Ratings cannot be negative\")\n",
    "    \n",
    "    max_user_idx = max(train_data['user_idx'].max(), test_data['user_idx'].max())\n",
    "    max_movie_idx = max(train_data['movie_idx'].max(), test_data['movie_idx'].max())\n",
    "    \n",
    "    if max_user_idx >= P.shape[0]:\n",
    "        raise ValueError(f\"User index {max_user_idx} out of bounds for P matrix with shape {P.shape}\")\n",
    "    if max_movie_idx >= Q.shape[0]:\n",
    "        raise ValueError(f\"Movie index {max_movie_idx} out of bounds for Q matrix with shape {Q.shape}\")\n",
    "\n",
    "def compute_rmse(data, P, Q):\n",
    "    \"\"\"Compute Root Mean Square Error\"\"\"\n",
    "    errors = []\n",
    "    for index, row in data.iterrows():\n",
    "        user = int(row['user_idx'])\n",
    "        item = int(row['movie_idx'])\n",
    "        rating = row['Rating']\n",
    "        \n",
    "        prediction = np.clip(np.dot(P[user, :], Q[item, :].T), 0.5, 5.0)\n",
    "        error = rating - prediction\n",
    "        errors.append(error**2)\n",
    "    \n",
    "    rmse = np.sqrt(np.mean(errors))\n",
    "    return rmse\n",
    "\n",
    "# Modified error handling in train_matrix_factorization\n",
    "def train_matrix_factorization(P, Q, train_data, test_data, num_factors, epochs=10, \n",
    "                             learning_rate=0.001, reg_param=0.02, early_stopping_rounds=3):\n",
    "    \"\"\"Train matrix factorization with early stopping and progress tracking\"\"\"\n",
    "    best_rmse = float('inf')\n",
    "    patience = early_stopping_rounds\n",
    "    patience_counter = 0\n",
    "    best_P, best_Q = P.copy(), Q.copy()  # Initialize with current values\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch: {epoch+1}\")\n",
    "            total_error = 0\n",
    "            n_samples = len(train_data)\n",
    "            \n",
    "            for index, row in tqdm(train_data.iterrows(), total=len(train_data), desc=\"Training\"):\n",
    "                user = int(row['user_idx'])\n",
    "                item = int(row['movie_idx'])\n",
    "                rating = float(row['Rating'])\n",
    "                \n",
    "                prediction = np.clip(np.dot(P[user, :], Q[item, :].T), 0.5, 5.0)\n",
    "                error = rating - prediction\n",
    "                \n",
    "                P[user, :] += learning_rate * (error * Q[item, :] - reg_param * P[user, :])\n",
    "                Q[item, :] += learning_rate * (error * P[user, :] - reg_param * Q[item, :])\n",
    "                \n",
    "                total_error += error ** 2\n",
    "            \n",
    "            train_rmse = np.sqrt(total_error / n_samples)\n",
    "            test_rmse = compute_rmse(test_data, P, Q)\n",
    "            \n",
    "            print(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "            print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "            \n",
    "            if test_rmse < best_rmse:\n",
    "                best_rmse = test_rmse\n",
    "                patience_counter = 0\n",
    "                best_P, best_Q = P.copy(), Q.copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"\\nEarly stopping triggered after epoch {epoch+1}\")\n",
    "                    return best_P, best_Q\n",
    "            \n",
    "            learning_rate *= 0.95\n",
    "        \n",
    "        return best_P, best_Q\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {str(e)}\")\n",
    "        # Return the best model we had before the error\n",
    "        return best_P, best_Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Matrix Factorization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model...\n",
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8000043/8000043 [29:30<00:00, 4518.82it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 3.1935\n",
      "Test RMSE: 3.1940\n",
      "Training completed and all files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Train Model and Save Results\n",
    "print(\"\\nTraining model...\")\n",
    "epochs = 1\n",
    "learning_rate = 0.001\n",
    "reg_param = 0.02\n",
    "early_stopping_rounds = 3\n",
    "\n",
    "# Define the path for saving\n",
    "save_path = r'C:\\Users\\anujp\\OneDrive\\Desktop\\MovieRecommendations\\models\\matrix_factorization\\stochastic_gradient_descent'\n",
    "\n",
    "try:\n",
    "    P_trained, Q_trained = train_matrix_factorization(\n",
    "        P=P,\n",
    "        Q=Q,\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "        num_factors=num_factors,\n",
    "        epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        reg_param=reg_param,\n",
    "        early_stopping_rounds=early_stopping_rounds\n",
    "    )\n",
    "    \n",
    "    # Save trained matrices\n",
    "    with open(os.path.join(save_path, 'P_trained.pkl'), 'wb') as f:\n",
    "        pickle.dump(P_trained, f)\n",
    "    \n",
    "    with open(os.path.join(save_path, 'Q_trained.pkl'), 'wb') as f:\n",
    "        pickle.dump(Q_trained, f)\n",
    "    \n",
    "    # Save all mappings\n",
    "    with open(os.path.join(save_path, 'user2idx.pkl'), 'wb') as f:\n",
    "        pickle.dump(user2idx, f)\n",
    "        \n",
    "    with open(os.path.join(save_path, 'movie2idx.pkl'), 'wb') as f:\n",
    "        pickle.dump(movie2idx, f)\n",
    "        \n",
    "    with open(os.path.join(save_path, 'idx2user.pkl'), 'wb') as f:\n",
    "        pickle.dump(idx2user, f)\n",
    "        \n",
    "    with open(os.path.join(save_path, 'idx2movie.pkl'), 'wb') as f:\n",
    "        pickle.dump(idx2movie, f)\n",
    "    \n",
    "    print(\"Training completed and all files saved successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Model on Test Data (MAE and RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model on test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing metrics: 100%|██████████| 1999982/1999982 [04:43<00:00, 7065.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 3.0127\n",
      "Test RMSE: 3.1940\n",
      "\n",
      "Computing additional metrics...\n",
      "Number of test samples: 1999982\n",
      "Rating range in test data: 0.5 to 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing predictions: 100%|██████████| 1999982/1999982 [01:50<00:00, 18045.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction statistics:\n",
      "Mean prediction: 0.5000\n",
      "Min prediction: 0.5000\n",
      "Max prediction: 0.6062\n",
      "Prediction std: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_load_path = r'C:\\Users\\anujp\\OneDrive\\Desktop\\MovieRecommendations\\models\\matrix_factorization\\stochastic_gradient_descent'\n",
    "\n",
    "# Check if the trained matrices are in the local scope, if not, try loading them\n",
    "if 'P_trained' not in locals() or 'Q_trained' not in locals():\n",
    "    print(\"Loading saved model...\")\n",
    "    try:\n",
    "        P_trained = np.load(f'{model_load_path}\\\\P_matrix.npy')\n",
    "        Q_trained = np.load(f'{model_load_path}\\\\Q_matrix.npy')\n",
    "    except FileNotFoundError:\n",
    "        print(\"Trained model files not found. Please ensure the model was saved correctly.\")\n",
    "        # Use the last state of P and Q if available, assuming P and Q are loaded or defined elsewhere\n",
    "        P_trained, Q_trained = P, Q\n",
    "\n",
    "# Modified compute_mae_rmse function with clipping\n",
    "def compute_mae_rmse(data, P, Q):\n",
    "    errors = []\n",
    "    abs_errors = []\n",
    "    for index, row in tqdm(data.iterrows(), total=len(data), desc=\"Computing metrics\"):\n",
    "        user = int(row['user_idx'])\n",
    "        item = int(row['movie_idx'])\n",
    "        rating = row['Rating']\n",
    "        \n",
    "        # Add clipping to match training\n",
    "        prediction = np.clip(np.dot(P[user, :], Q[item, :].T), 0.5, 5.0)\n",
    "        error = rating - prediction\n",
    "        errors.append(error**2)\n",
    "        abs_errors.append(abs(error))\n",
    "    \n",
    "    rmse = np.sqrt(np.mean(errors))\n",
    "    mae = np.mean(abs_errors)\n",
    "    return mae, rmse\n",
    "\n",
    "# Evaluate on test data\n",
    "try:\n",
    "    print(\"\\nEvaluating model on test data...\")\n",
    "    mae, rmse = compute_mae_rmse(test_data, P_trained, Q_trained)\n",
    "    print(f\"Test MAE: {mae:.4f}\")\n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # Additional evaluation metrics\n",
    "    print(\"\\nComputing additional metrics...\")\n",
    "    print(f\"Number of test samples: {len(test_data)}\")\n",
    "    print(f\"Rating range in test data: {test_data['Rating'].min():.1f} to {test_data['Rating'].max():.1f}\")\n",
    "    \n",
    "    # Compute prediction statistics\n",
    "    predictions = []\n",
    "    for index, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Computing predictions\"):\n",
    "        user = int(row['user_idx'])\n",
    "        item = int(row['movie_idx'])\n",
    "        pred = np.clip(np.dot(P_trained[user, :], Q_trained[item, :].T), 0.5, 5.0)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    print(f\"\\nPrediction statistics:\")\n",
    "    print(f\"Mean prediction: {predictions.mean():.4f}\")\n",
    "    print(f\"Min prediction: {predictions.min():.4f}\")\n",
    "    print(f\"Max prediction: {predictions.max():.4f}\")\n",
    "    print(f\"Prediction std: {predictions.std():.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during evaluation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Precision@K and Recall@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_recommendations(P, Q, user_idx, train_data, K=10):\n",
    "    user_rated_items = train_data[train_data['user_idx'] == user_idx]['movie_idx'].tolist()\n",
    "    scores = np.dot(Q, P[user_idx, :])\n",
    "    # Exclude items already rated by user\n",
    "    scores[user_rated_items] = -np.inf\n",
    "    top_k_items = np.argsort(-scores)[:K]\n",
    "    return top_k_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing Precision and Recall metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Precision/Recall:  59%|█████▉    | 41393/69796 [10:54<07:11, 65.88it/s]"
     ]
    }
   ],
   "source": [
    "def compute_precision_recall_at_k(test_data, train_data, P, Q, K=10, threshold=4.0):\n",
    "    \"\"\"Compute Precision@K and Recall@K\"\"\"\n",
    "    user_precisions = []\n",
    "    user_recalls = []\n",
    "    users_in_test = test_data['user_idx'].unique()\n",
    "    \n",
    "    for user in tqdm(users_in_test, desc=\"Computing Precision/Recall\"):\n",
    "        # Get actual items the user liked in test data\n",
    "        actual_items = test_data[(test_data['user_idx'] == user) & \n",
    "                               (test_data['Rating'] >= threshold)]['movie_idx'].tolist()\n",
    "        if not actual_items:\n",
    "            continue\n",
    "            \n",
    "        # Get top K recommendations\n",
    "        recommended_items = get_top_k_recommendations(P, Q, user, train_data, K)\n",
    "        \n",
    "        # Compute hits\n",
    "        hits = set(actual_items) & set(recommended_items)\n",
    "        precision = len(hits) / K\n",
    "        recall = len(hits) / len(actual_items)\n",
    "        \n",
    "        user_precisions.append(precision)\n",
    "        user_recalls.append(recall)\n",
    "    \n",
    "    # Compute average precision and recall\n",
    "    avg_precision = np.mean(user_precisions)\n",
    "    avg_recall = np.mean(user_recalls)\n",
    "    \n",
    "    return avg_precision, avg_recall\n",
    "\n",
    "try:\n",
    "    # First ensure we have the trained matrices\n",
    "    if 'P_trained' not in locals() or 'Q_trained' not in locals():\n",
    "        print(\"Loading saved model...\")\n",
    "        P_trained = np.load('model/P_matrix.npy')\n",
    "        Q_trained = np.load('model/Q_matrix.npy')\n",
    "\n",
    "    # Compute Precision@K and Recall@K\n",
    "    print(\"\\nComputing Precision and Recall metrics...\")\n",
    "    precision_at_k, recall_at_k = compute_precision_recall_at_k(\n",
    "        test_data=test_data,\n",
    "        train_data=train_data,\n",
    "        P=P_trained,\n",
    "        Q=Q_trained,\n",
    "        K=10,\n",
    "        threshold=4.0\n",
    "    )\n",
    "    \n",
    "    print(f\"Precision@10: {precision_at_k:.4f}\")\n",
    "    print(f\"Recall@10: {recall_at_k:.4f}\")\n",
    "    \n",
    "    # Additional metrics\n",
    "    print(\"\\nDetailed metrics:\")\n",
    "    print(f\"Number of users evaluated: {len(test_data['user_idx'].unique())}\")\n",
    "    print(f\"Number of items evaluated: {len(test_data['movie_idx'].unique())}\")\n",
    "    print(f\"Rating threshold for 'liked' items: {4.0}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during metric computation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Compute Normalized Discounted Cumulative Gain (NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(rel_scores, K):\n",
    "    rel_scores = np.array(rel_scores)[:K]\n",
    "    dcg = np.sum((2 ** rel_scores - 1) / np.log2(np.arange(2, rel_scores.size + 2)))\n",
    "    return dcg\n",
    "\n",
    "def idcg_at_k(rel_scores, K):\n",
    "    sorted_scores = sorted(rel_scores, reverse=True)\n",
    "    idcg = dcg_at_k(sorted_scores, K)\n",
    "    return idcg\n",
    "\n",
    "def compute_ndcg_at_k(test_data, train_data, P, Q, K=10):\n",
    "    user_ndcgs = []\n",
    "    users_in_test = test_data['user_idx'].unique()\n",
    "    for user in users_in_test:\n",
    "        # Get actual items and ratings the user has in test data\n",
    "        user_test_data = test_data[test_data['user_idx'] == user]\n",
    "        actual_items = user_test_data['movie_idx'].tolist()\n",
    "        actual_ratings = user_test_data['Rating'].tolist()\n",
    "        if not actual_items:\n",
    "            continue\n",
    "        # Get top K recommendations\n",
    "        recommended_items = get_top_k_recommendations(P, Q, user, train_data, K)\n",
    "        # Get relevance scores\n",
    "        rel_scores = []\n",
    "        for item in recommended_items:\n",
    "            if item in actual_items:\n",
    "                rating = user_test_data[user_test_data['movie_idx'] == item]['Rating'].values[0]\n",
    "                rel = rating / 5.0  # Normalize rating to [0,1]\n",
    "            else:\n",
    "                rel = 0\n",
    "            rel_scores.append(rel)\n",
    "        dcg = dcg_at_k(rel_scores, K)\n",
    "        idcg = idcg_at_k(rel_scores, K)\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "        user_ndcgs.append(ndcg)\n",
    "    avg_ndcg = np.mean(user_ndcgs)\n",
    "    return avg_ndcg\n",
    "\n",
    "# Compute NDCG@K\n",
    "ndcg_at_k = compute_ndcg_at_k(test_data, train_data, P_trained, Q_trained, K=10)\n",
    "print(f\"NDCG@10: {ndcg_at_k:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model directory if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists('model'):\n",
    "    os.makedirs('model')\n",
    "#idx2user.pkl,idx2movie,movie2idx\n",
    "# Create reverse mappings\n",
    "idx2user = {idx: user_id for user_id, idx in user2idx.items()}\n",
    "idx2movie = {idx: movie_id for movie_id, idx in movie2idx.items()}\n",
    "\n",
    "try:\n",
    "    # Save user and item latent factors\n",
    "    with open('model/P_trained.pkl', 'wb') as f:\n",
    "        pickle.dump(P_trained, f)\n",
    "\n",
    "    with open('model/Q_trained.pkl', 'wb') as f:\n",
    "        pickle.dump(Q_trained, f)\n",
    "\n",
    "    # Save user and movie mappings\n",
    "    with open('model/user2idx.pkl', 'wb') as f:\n",
    "        pickle.dump(user2idx, f)\n",
    "\n",
    "    with open('model/idx2user.pkl', 'wb') as f:\n",
    "        pickle.dump(idx2user, f)\n",
    "\n",
    "    with open('model/movie2idx.pkl', 'wb') as f:\n",
    "        pickle.dump(movie2idx, f)\n",
    "\n",
    "    with open('model/idx2movie.pkl', 'wb') as f:\n",
    "        pickle.dump(idx2movie, f)\n",
    "\n",
    "    # Verify saved files\n",
    "    saved_files = ['P_trained.pkl', 'Q_trained.pkl', 'user2idx.pkl', \n",
    "                   'idx2user.pkl', 'movie2idx.pkl', 'idx2movie.pkl']\n",
    "    all_files_saved = all(os.path.exists(f'model/{file}') for file in saved_files)\n",
    "    \n",
    "    if all_files_saved:\n",
    "        print(\"Model and mappings have been saved successfully.\")\n",
    "    else:\n",
    "        print(\"Warning: Some files may not have been saved properly.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, movie_id, P, Q, user2idx, movie2idx):\n",
    "   if user_id in user2idx and movie_id in movie2idx:\n",
    "       user_idx = user2idx[user_id]\n",
    "       movie_idx = movie2idx[movie_id]\n",
    "       prediction = np.dot(P[user_idx, :], Q[movie_idx, :].T)\n",
    "       # Clip the prediction to the rating scale (e.g., 1 to 5)\n",
    "       prediction = min(max(prediction, 1), 5)\n",
    "       return prediction\n",
    "   else:\n",
    "       return np.nan  # User or movie not in training data\n",
    "\n",
    "# Example usage\n",
    "# Choose any specific user ID\n",
    "sample_user_id = 42  # Replace with any user ID you want\n",
    "sample_movie_id = df['MovieID'].iloc[0]\n",
    "\n",
    "predicted_rating = predict_rating(sample_user_id, sample_movie_id, P_trained, Q_trained, user2idx, movie2idx)\n",
    "print(f\"Predicted rating for user {sample_user_id} and movie {sample_movie_id}: {predicted_rating:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Top-K Recommendations for a User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_top_k_movies(user_id, P, Q, user2idx, idx2movie, train_data, df, K=10):\n",
    "    \"\"\"\n",
    "    Recommend top K movies for a user with their titles\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_id : int\n",
    "        The ID of the user to get recommendations for\n",
    "    P : numpy array\n",
    "        User latent factors matrix\n",
    "    Q : numpy array\n",
    "        Movie latent factors matrix\n",
    "    user2idx : dict\n",
    "        Mapping from user ID to matrix index\n",
    "    idx2movie : dict\n",
    "        Mapping from matrix index to movie ID\n",
    "    train_data : pandas DataFrame\n",
    "        Training data containing user-movie interactions\n",
    "    df : pandas DataFrame\n",
    "        Original dataframe containing movie titles\n",
    "    K : int, default=10\n",
    "        Number of recommendations to return\n",
    "    \"\"\"\n",
    "    if user_id in user2idx:\n",
    "        user_idx = user2idx[user_id]\n",
    "        # Get movies the user has already rated\n",
    "        user_rated_movies = train_data[train_data['user_idx'] == user_idx]['movie_idx'].tolist()\n",
    "        # Predict scores for all movies\n",
    "        scores = np.dot(Q, P[user_idx, :])\n",
    "        # Exclude movies the user has already rated\n",
    "        scores[user_rated_movies] = -np.inf\n",
    "        # Get top K movie indices\n",
    "        top_k_movie_indices = np.argsort(-scores)[:K]\n",
    "        # Map indices to movie IDs\n",
    "        top_k_movie_ids = [idx2movie[idx] for idx in top_k_movie_indices]\n",
    "        \n",
    "        # Get predicted ratings\n",
    "        recommendations = []\n",
    "        for movie_id in top_k_movie_ids:\n",
    "            # Get movie title\n",
    "            movie_title = df[df['MovieID'] == movie_id]['Title'].iloc[0]\n",
    "            # Get predicted rating\n",
    "            movie_idx = movie2idx[movie_id]\n",
    "            predicted_rating = np.clip(np.dot(P[user_idx, :], Q[movie_idx, :].T), 0.5, 5.0)\n",
    "            recommendations.append((movie_id, movie_title, predicted_rating))\n",
    "        \n",
    "        return recommendations\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Choose any specific user ID\n",
    "user_id_to_recommend = 42  # Replace with any user ID you want\n",
    "recommendations = recommend_top_k_movies(\n",
    "    user_id=user_id_to_recommend,\n",
    "    P=P_trained,\n",
    "    Q=Q_trained,\n",
    "    user2idx=user2idx,\n",
    "    idx2movie=idx2movie,\n",
    "    train_data=train_data,\n",
    "    df=df,\n",
    "    K=10\n",
    ")\n",
    "\n",
    "\n",
    "# Print recommendations\n",
    "print(f\"\\nTop 10 movie recommendations for user {user_id_to_recommend}:\")\n",
    "if recommendations:\n",
    "    for i, (movie_id, title, pred_rating) in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {title} (ID: {movie_id}) - Predicted Rating: {pred_rating:.2f}\")\n",
    "else:\n",
    "    print(\"No recommendations available for this user.\")\n",
    "\n",
    "# Optional: Print the user's actual ratings for comparison\n",
    "print(f\"\\nMovies this user has already rated:\")\n",
    "user_ratings = df[df['UserID'] == user_id_to_recommend].sort_values('Rating', ascending=False)\n",
    "for _, row in user_ratings.head().iterrows():\n",
    "    print(f\"- {row['Title']} - Actual Rating: {row['Rating']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# TMDb API configuration\n",
    "TMDB_API_KEY = \"165b9xxxxxx\"\n",
    "TMDB_BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "\n",
    "def load_saved_model():\n",
    "    \"\"\"Load the saved model and mappings\"\"\"\n",
    "    try:\n",
    "        with open('model/P_trained.pkl', 'rb') as f:\n",
    "            P_trained = pickle.load(f)\n",
    "        with open('model/Q_trained.pkl', 'rb') as f:\n",
    "            Q_trained = pickle.load(f)\n",
    "        with open('model/user2idx.pkl', 'rb') as f:\n",
    "            user2idx = pickle.load(f)\n",
    "        with open('model/idx2movie.pkl', 'rb') as f:\n",
    "            idx2movie = pickle.load(f)\n",
    "        with open('model/movie2idx.pkl', 'rb') as f:\n",
    "            movie2idx = pickle.load(f)\n",
    "            \n",
    "        return P_trained, Q_trained, user2idx, idx2movie, movie2idx\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_movie_info(movie_title):\n",
    "    \"\"\"Get movie information from TMDb API\"\"\"\n",
    "    # Search for the movie\n",
    "    search_url = f\"{TMDB_BASE_URL}/search/movie\"\n",
    "    params = {\n",
    "        'api_key': TMDB_API_KEY,\n",
    "        'query': movie_title\n",
    "    }\n",
    "    response = requests.get(search_url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    if data['results']:\n",
    "        movie = data['results'][0]\n",
    "        poster_path = movie['poster_path']\n",
    "        if poster_path:\n",
    "            poster_url = f\"https://image.tmdb.org/t/p/w500{poster_path}\"\n",
    "            return movie['title'], poster_url, movie['overview'], movie['release_date']\n",
    "    return None, None, None, None\n",
    "\n",
    "def plot_recommendations_with_posters(user_id, df, P_trained, Q_trained, user2idx, idx2movie, movie2idx, K=5):\n",
    "    \"\"\"Plot top K movie recommendations with posters\"\"\"\n",
    "    if user_id not in user2idx:\n",
    "        print(\"User not found in the dataset\")\n",
    "        return\n",
    "    \n",
    "    user_idx = user2idx[user_id]\n",
    "    scores = np.dot(Q_trained, P_trained[user_idx, :])\n",
    "    top_k_indices = np.argsort(-scores)[:K]\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    plt.suptitle(f'Top {K} Movie Recommendations for User {user_id}', fontsize=16)\n",
    "    \n",
    "    for i, idx in enumerate(top_k_indices, 1):\n",
    "        movie_id = idx2movie[idx]\n",
    "        movie_title = df[df['MovieID'] == movie_id]['Title'].iloc[0]\n",
    "        predicted_rating = scores[idx]\n",
    "        \n",
    "        # Get movie info from TMDb\n",
    "        title, poster_url, overview, release_date = get_movie_info(movie_title)\n",
    "        \n",
    "        # Create subplot\n",
    "        ax = plt.subplot(1, K, i)\n",
    "        \n",
    "        if poster_url:\n",
    "            # Display poster\n",
    "            response = requests.get(poster_url)\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Add title and rating\n",
    "            ax.set_title(f\"{title}\\nPred Rating: {predicted_rating:.2f}\", \n",
    "                        fontsize=10, pad=5)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f\"No poster available\\n{movie_title}\\nRating: {predicted_rating:.2f}\",\n",
    "                   ha='center', va='center')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_user_rating_distribution(user_id, df):\n",
    "    \"\"\"Plot rating distribution for a specific user\"\"\"\n",
    "    user_ratings = df[df['UserID'] == user_id]['Rating']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(user_ratings, bins=10)\n",
    "    plt.title(f'Rating Distribution for User {user_id}')\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nRating Statistics for User {user_id}:\")\n",
    "    print(f\"Average Rating: {user_ratings.mean():.2f}\")\n",
    "    print(f\"Number of Ratings: {len(user_ratings)}\")\n",
    "\n",
    "# Load the saved model and original dataset\n",
    "P_trained, Q_trained, user2idx, idx2movie, movie2idx = load_saved_model()\n",
    "df = pd.read_csv(\"C:/Users/anujp/OneDrive/Desktop/MovieRecommendations/data/Final_data/Final_data.csv\")\n",
    "\n",
    "# Example usage\n",
    "user_id = 42  # You can change this to any user ID\n",
    "plot_recommendations_with_posters(user_id, df, P_trained, Q_trained, user2idx, idx2movie, movie2idx)\n",
    "plot_user_rating_distribution(user_id, df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
